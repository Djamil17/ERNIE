{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import swifter  # Makes applying to datframe as fast as vectorizing\n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from textblob import TextBlob, Word\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# ------------------------------------------------------------------------------------ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "f = open('/Users/shreya/nih_stopwords.txt', 'r')\n",
    "stop_words.extend([word.rstrip('\\n') for word in f])\n",
    "f.close()\n",
    "stop_words.extend(['a', 'in', 'of', 'the', 'at', 'on', 'and', 'with', 'from', 'to', 'for',\n",
    "                   'some', 'but', 'not', 'their', 'human', 'mouse', 'rat', 'monkey', 'man',\n",
    "                   'method', 'during', 'process', 'hitherto', 'unknown', 'many', 'these',\n",
    "                   'have', 'into', 'improved', 'use', 'find', 'show', 'may', 'study', 'result',\n",
    "                   'contain', 'day', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight',\n",
    "                   'nine', 'ten', 'give', 'also', 'suggest', 'data', 'number', 'right reserved',\n",
    "                   'right', 'reserve', 'society', 'american', 'publish', 'group', 'wiley', 'depend',\n",
    "                   'upon', 'good', 'within', 'small', 'amount', 'large', 'quantity', 'control',\n",
    "                   'complete', 'record', 'task', 'effect', 'single', 'database', 'ref',\n",
    "                   'ref', 'university', 'press', 'psycinfo', 'apa', 'inc', 'alan', 'find', 'finding',\n",
    "                   'perform', 'new', 'reference', 'noticeable', 'america', 'copyright', 'attempt', 'make',\n",
    "                   'theory', 'demonstrate', 'present', 'analysis', 'other', 'due', 'receive', 'rabbit',\n",
    "                   'property', 'e.g.', 'and/or', 'or', 'unlikely', 'nih', 'cause', 'best', 'well',\n",
    "                   'without', 'whereas', 'whatever', 'require', 'wiley', 'aaa', 'whether', 'require',\n",
    "                   'relevant', 'take', 'together', 'appear'])  # ---> updated based on results\n",
    "\n",
    "\n",
    "def preprocess_text(doc, n_grams='two'):\n",
    "    \"\"\"\n",
    "    Pre-processing using TextBlob: \n",
    "    tokenizing, converting to lower-case, and lemmatization based on POS tagging, \n",
    "    removing stop-words, and retaining tokens greater than length 2\n",
    "\n",
    "    We can also choose to include n_grams (n = 1,2,3) in the final output\n",
    "\n",
    "    Argument(s): 'doc' - a string of words or sentences.\n",
    "                 'n_grams' - one: only unigrams (tokens consisting of one word each)\n",
    "                           - two: only bigrams\n",
    "                           - two_plus: unigrams + bigrams\n",
    "                           - three: only trigrams \n",
    "                           - three_plus: unigrams + bigrams + trigrams\n",
    "\n",
    "    Output: 'reuslt_singles' - a list of pre-processed tokens (individual words) of each sentence in 'doc'\n",
    "            'result_ngrams' - a list of pre-processed tokens (including n-grams) of each sentence in 'doc'\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    blob = TextBlob(doc).lower()\n",
    "#     lang = blob.detect_language()\n",
    "#     print(lang)\n",
    "#     if lang != 'en':\n",
    "#         blob = blob.translate(to = 'en')\n",
    "\n",
    "    result_singles = []\n",
    "\n",
    "    tag_dict = {\"J\": 'a',  # Adjective\n",
    "                \"N\": 'n',  # Noun\n",
    "                \"V\": 'v',  # Verb\n",
    "                \"R\": 'r'}  # Adverb\n",
    "\n",
    "    # For all other types of parts of speech (including those not classified at all)\n",
    "    # the tag_dict object maps to 'None'\n",
    "    # the method w.lemmatize() defaults to 'Noun' as POS for those classified as 'None'\n",
    "\n",
    "    for sent in blob.sentences:\n",
    "\n",
    "        words_and_tags = [(w, tag_dict.get(pos[0])) for w, pos in sent.tags]\n",
    "        lemmatized_list = [w.lemmatize(tag) for w, tag in words_and_tags]\n",
    "\n",
    "        for i in range(len(lemmatized_list)):\n",
    "\n",
    "            if lemmatized_list[i] not in stop_words and len(lemmatized_list[i].lower()) > 2 and not lemmatized_list[i].isdigit():\n",
    "                result_singles.append(lemmatized_list[i].lower())\n",
    "\n",
    "    result_bigrams = ['_'.join(x) for x in ngrams(result_singles, 2)]\n",
    "\n",
    "    result_bigrams = [\n",
    "        token for token in result_bigrams if token != 'psychological_association']\n",
    "\n",
    "    result_trigrams = ['_'.join(x) for x in ngrams(result_singles, 3)]\n",
    "    result_two_plus = result_singles + result_bigrams\n",
    "    result_three_plus = result_singles + result_bigrams + result_trigrams\n",
    "\n",
    "    if n_grams == 'one':\n",
    "        result = result_singles\n",
    "    elif n_grams == 'two':\n",
    "        result = result_bigrams\n",
    "    elif n_grams == 'three':\n",
    "        result = result_trigrams\n",
    "    elif n_grams == 'two_plus':\n",
    "        result = result_two_plus\n",
    "    elif n_grams == 'three_plus':\n",
    "        result = result_three_plus\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequency(processed_text_list):\n",
    "    \"\"\"\n",
    "    Using a built-in NLTK function that generates tuples\n",
    "    We get the frequency distribution of all words/n-grams in a tokenized list\n",
    "    We can get the proportion of the the token as a fraction of the total corpus size  ----> N/A\n",
    "    We can also sort these frequencies and proportions in descending order in a dictionary object ----> N/A\n",
    "\n",
    "    Argument(s): 'processed_text_list' - A list of pre-processed tokens\n",
    "\n",
    "    Output(s): freq_dict - A dictionary of tokens and their respective frequencies in descending order\n",
    "    \"\"\"\n",
    "    # prop_dict - A dictionary of tokens and their respective proportions as a fraction of the total corpus\n",
    "    # combined_dict - A dictionary whose values are both frequencies and proportions combined within a list\n",
    "    # \"\"\"\n",
    "\n",
    "    word_frequency = FreqDist(word for word in processed_text_list)\n",
    "\n",
    "#     sorted_counts = sorted(word_frequency.items(), key = lambda x: x[1], reverse = True)\n",
    "#     freq_dict = dict(sorted_counts)\n",
    "    freq_dict = dict(word_frequency)\n",
    "#     prop_dict = {key : freq_dict[key] * 1.0 / sum(freq_dict.values()) for key, value in freq_dict.items()}\n",
    "#     combined_dict = {key : [freq_dict[key], freq_dict[key] * 1.0 / sum(freq_dict.values())] for key, value in freq_dict.items()}\n",
    "\n",
    "    return freq_dict  # , prop_dict, combined_dict\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab_dictionary(vocab_column):\n",
    "    \"\"\"\n",
    "    Takes any number of token frequency dictionaries and merges them while summing \n",
    "    the respective frequencies and then calculates the proportion of the the tokens \n",
    "    as a fraction of the total corpus size and saves to text and CSV files\n",
    "\n",
    "\n",
    "    Argument(s): vocab_column - A column of dictionary objects\n",
    "\n",
    "    Output(s): merged_combined_dict - A list object containing the frequencies of all\n",
    "               merged dictionary tokens along with their respective proportions\n",
    "    \"\"\"\n",
    "\n",
    "    merged_freq_dict = {}\n",
    "    for dictionary in vocab_column:\n",
    "        for key, value in dictionary.items():  # d.items() in Python 3+\n",
    "            merged_freq_dict.setdefault(key, []).append(1)\n",
    "\n",
    "    for key, value in merged_freq_dict.items():\n",
    "        merged_freq_dict[key] = sum(value)\n",
    "\n",
    "    sorted_merged_freq_dict = sorted(\n",
    "        merged_freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#     total_sum = sum(merged_freq_dict.values())\n",
    "#     merged_prop_dict = {key : merged_freq_dict[key] * 1.0 / total_sum for key, value in merged_freq_dict.items()}\n",
    "#     merged_combined_dict = {key : [merged_freq_dict[key], (merged_freq_dict[key] * 1.0 / total_sum)] for key, value in merged_freq_dict.items()}\n",
    "\n",
    "    return sorted_merged_freq_dict\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_term_mat(corpus_by_article, corpus_by_cluster):\n",
    "    \n",
    "    \"\"\"\n",
    "    Argument(s): corpus_by_article - (list of lists)\n",
    "                 corpus_by_cluster - flattened list \n",
    "\n",
    "    Output(s): doc_term_prob_mat, cluster_prob_mat\n",
    "    \"\"\"\n",
    "\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    cluster_count_mat = count_vectorizer.fit_transform(corpus_by_cluster)\n",
    "    doc_term_count_mat = count_vectorizer.transform(corpus_by_article)\n",
    "\n",
    "    prob_transformer = TfidfTransformer(norm='l1', use_idf=False, smooth_idf=False)\n",
    "    \n",
    "    doc_term_prob_mat = prob_transformer.fit_transform(doc_term_count_mat)\n",
    "    cluster_prob_mat = prob_transformer.fit_transform(cluster_count_mat)\n",
    "    \n",
    "    return doc_term_prob_mat, cluster_prob_mat\n",
    "\n",
    "# ------------------------------------------------------------------------------------ #\n",
    "# ------------------------------------------------------------------------------------ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------ #\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "with open('/Users/shreya/Documents/ernie_password.txt') as f:\n",
    "    ernie_password = f.readline()\n",
    "\n",
    "conn=psycopg2.connect(database=\"ernie\",user=\"shreya\",host=\"localhost\",password=ernie_password)\n",
    "conn.set_client_encoding('UTF8')\n",
    "conn.autocommit=True\n",
    "curs=conn.cursor()\n",
    "\n",
    "# Set schema\n",
    "schema = \"theta_plus\"\n",
    "set_schema = \"SET SEARCH_PATH TO \" + schema + \";\"   \n",
    "curs.execute(set_schema)\n",
    "\n",
    "\n",
    "weights = ['ncf', 'now', 'sf'] \n",
    "inflation = ['20', '30', '40', '60']\n",
    "        \n",
    "title_abstract_table = 'top_scp_title_concat_abstract_english'\n",
    "\n",
    "\n",
    "jsd_output = pd.DataFrame()\n",
    "\n",
    "for name in weights:\n",
    "    for val in inflation:\n",
    "        cluster_table = name + '_' + val + '_ids'\n",
    "        print(\"Querying: \", cluster_table)\n",
    "        \n",
    "        \n",
    "        max_query = \"SELECT MAX(cluster) FROM \" + cluster_table + \";\"\n",
    "\n",
    "        curs.execute(max_query, conn)\n",
    "        max_cluster_number = curs.fetchone()[0]\n",
    "\n",
    "        for cluster_num in range(1, max_cluster_number+1):\n",
    "            \n",
    "            print(\"Querying Cluster Number: \", str(cluster_num))\n",
    "            \n",
    "            \n",
    "            query = \"SELECT clt.scp, tat.title, tat.abstract_text, clt.cluster \" + \"FROM \" + cluster_table +  \" AS clt \" + \"LEFT JOIN \" + title_abstract_table + \" AS tat \" + \"ON clt.scp = tat.scp \" + \"WHERE clt.cluster=\" + str(cluster_num) + \";\"\n",
    "\n",
    "            print(query)\n",
    "\n",
    "\n",
    "            mcl_data = pd.read_sql(query, conn)\n",
    "\n",
    "            print('The size of the cluster is: ', len(mcl_data))\n",
    "            original_cluster_size = len(mcl_data)\n",
    "            mcl_data = mcl_data.dropna()\n",
    "            print('Size after removing missing titles and abstracts: ', len(mcl_data))\n",
    "            final_cluster_size = len(mcl_data)\n",
    "            \n",
    "            \n",
    "            if final_cluster_size < 10:\n",
    "                \n",
    "                print(\"\")\n",
    "                print(\"The cluster size is inadequate.\")\n",
    "                print(\"Moving to next cluster.\")\n",
    "                print(\"\")\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                data_text = mcl_data.copy()\n",
    "                data_text['scp'] = data_text.astype('str')\n",
    "                data_text['processed_title'] = data_text['title'].swifter.apply(preprocess_text)\n",
    "                data_text['processed_abstract'] = data_text['abstract_text'].swifter.apply(preprocess_text)\n",
    "\n",
    "                data_text['processed_all_text'] = data_text['processed_title'] + data_text['processed_abstract']\n",
    "                data_text['processed_all_text_frequencies'] = data_text['processed_all_text'].swifter.apply(get_frequency)\n",
    "                data_all_text_frequency = merge_vocab_dictionary(data_text['processed_all_text_frequencies'])\n",
    "\n",
    "                mcl_all_text = data_text.processed_all_text.tolist()\n",
    "                corpus_by_article = [' '.join(text) for text in mcl_all_text]\n",
    "                corpus_by_cluster = [' '.join(corpus_by_article)]\n",
    "\n",
    "                doc_term_prob_mat, cluster_prob_mat = get_doc_term_mat(corpus_by_article, corpus_by_cluster)\n",
    "                cluster_prob_vec = cluster_prob_mat.toarray().tolist()[0]\n",
    "\n",
    "                data_text['doc_term_prob'] = doc_term_prob_mat.toarray().tolist()\n",
    "\n",
    "                def calculate_jsd(doc_prob_vec, cluster_prob_vec = cluster_prob_vec):\n",
    "\n",
    "                    jsd = distance.jensenshannon(doc_prob_vec, cluster_prob_vec)\n",
    "\n",
    "                    return jsd\n",
    "\n",
    "\n",
    "                data_text['JSD'] = data_text['doc_term_prob'].swifter.apply(calculate_jsd)\n",
    "                \n",
    "                jsd_min = min(data_text['JSD'])\n",
    "                jsd_25_percentile = np.percentile(data_text['JSD'], 25)\n",
    "                jsd_median = np.percentile(data_text['JSD'], 50)\n",
    "                jsd_75_percentile = np.percentile(data_text['JSD'], 75)\n",
    "                jsd_max = max(data_text['JSD'])\n",
    "                jsd_mean = np.mean(data_text['JSD'])\n",
    "                jsd_std = np.std(data_text['JSD'])\n",
    "                \n",
    "#                 print(\"\")\n",
    "#                 print('Minimum JSD value for the cluster is: ', jsd_min)\n",
    "#                 print('25th Percentile JSD value for the cluster is: ', jsd_25_percentile)\n",
    "#                 print('Median JSD value for the cluster is: ', jsd_median)\n",
    "#                 print('75th Percentile JSD value for the cluster is: ', jsd_75_percentile)\n",
    "#                 print('Maximum JSD value for the cluster is: ', jsd_max)\n",
    "#                 print('Mean JSD value for the cluster is: ', jsd_mean)\n",
    "#                 print('Std Dev of JSD for the cluster is: ', np.std(data_text['JSD']))\n",
    "#                 print(\"\")\n",
    "\n",
    "                \n",
    "                print(\"\")\n",
    "                print(\"JSD computed.\")\n",
    "                print(\"Cluster analysis complete.\")\n",
    "                print(\"\")\n",
    "                print(\"Saving to CSV\")\n",
    "                print(\"\")\n",
    "                \n",
    "                result_df = pd.DataFrame({\n",
    "                    'Weight': name, \n",
    "                    'Inflation': val,\n",
    "                    'Cluster': cluster_num,\n",
    "                    'Total Size': original_cluster_size, \n",
    "                    'Final Size': final_cluster_size, \n",
    "                    'Missing Values': (original_cluster_size-final_cluster_size,), \n",
    "                    'Mean JSD': jsd_mean, \n",
    "                    'Min JSD': jsd_min,\n",
    "                    '25th Percentile': jsd_25_percentile, \n",
    "                    'Median JSD': jsd_median,\n",
    "                    '75th Percentile': jsd_75_percentile, \n",
    "                    'Max JSD': jsd_max, \n",
    "                    'Std Dev':jsd_std})\n",
    "                \n",
    "                jsd_output = jsd_output.append(result_df)\n",
    "                jsd_output.to_csv(\"/Users/shreya/Documents/JSD_output.csv\", index = None, header=True, encoding='utf-8')\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "print(\"ALL COMPLETED.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
