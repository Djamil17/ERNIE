{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-08T07:02:34.276742Z",
     "iopub.status.busy": "2020-05-08T07:02:34.276495Z",
     "iopub.status.idle": "2020-05-08T07:02:40.475563Z",
     "shell.execute_reply": "2020-05-08T07:02:40.474806Z",
     "shell.execute_reply.started": "2020-05-08T07:02:34.276697Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shreya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import swifter  # Makes applying to datframe as fast as vectorizing\n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from textblob import TextBlob, Word\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "from sys import argv\n",
    "from scipy import sparse\n",
    "\n",
    "# ------------------------------------------------------------------------------------ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-08T07:02:41.675435Z",
     "iopub.status.busy": "2020-05-08T07:02:41.675215Z",
     "iopub.status.idle": "2020-05-08T07:02:41.711609Z",
     "shell.execute_reply": "2020-05-08T07:02:41.710755Z",
     "shell.execute_reply.started": "2020-05-08T07:02:41.675402Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "f = open('/Users/shreya/nih_stopwords.txt', 'r')\n",
    "stop_words.extend([word.rstrip('\\n') for word in f])\n",
    "f.close()\n",
    "stop_words.extend(['a', 'in', 'of', 'the', 'at', 'on', 'and', 'with', 'from', 'to', 'for',\n",
    "                   'some', 'but', 'not', 'their', 'human', 'mouse', 'rat', 'monkey', 'man',\n",
    "                   'method', 'during', 'process', 'hitherto', 'unknown', 'many', 'these',\n",
    "                   'have', 'into', 'improved', 'use', 'find', 'show', 'may', 'study', 'result',\n",
    "                   'contain', 'day', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight',\n",
    "                   'nine', 'ten', 'give', 'also', 'suggest', 'data', 'number', 'right reserved',\n",
    "                   'right', 'reserve', 'society', 'american', 'publish', 'group', 'wiley', 'depend',\n",
    "                   'upon', 'good', 'within', 'small', 'amount', 'large', 'quantity', 'control',\n",
    "                   'complete', 'record', 'task', 'effect', 'single', 'database', 'ref',\n",
    "                   'ref', 'university', 'press', 'psycinfo', 'apa', 'inc', 'alan', 'find', 'finding',\n",
    "                   'perform', 'new', 'reference', 'noticeable', 'america', 'copyright', 'attempt', 'make',\n",
    "                   'theory', 'demonstrate', 'present', 'analysis', 'other', 'due', 'receive', 'rabbit',\n",
    "                   'property', 'e.g.', 'and/or', 'or', 'unlikely', 'nih', 'cause', 'best', 'well',\n",
    "                   'without', 'whereas', 'whatever', 'require', 'wiley', 'aaa', 'whether', 'require',\n",
    "                   'relevant', 'take', 'together', 'appear'])  # ---> updated based on results\n",
    "\n",
    "\n",
    "def preprocess_text(doc, n_grams='two'):\n",
    "    \"\"\"\n",
    "    Pre-processing using TextBlob: \n",
    "    tokenizing, converting to lower-case, and lemmatization based on POS tagging, \n",
    "    removing stop-words, and retaining tokens greater than length 2\n",
    "\n",
    "    We can also choose to include n_grams (n = 1,2,3) in the final output\n",
    "\n",
    "    Argument(s): 'doc' - a string of words or sentences.\n",
    "                 'n_grams' - one: only unigrams (tokens consisting of one word each)\n",
    "                           - two: only bigrams\n",
    "                           - two_plus: unigrams + bigrams\n",
    "                           - three: only trigrams \n",
    "                           - three_plus: unigrams + bigrams + trigrams\n",
    "\n",
    "    Output: 'reuslt_singles' - a list of pre-processed tokens (individual words) of each sentence in 'doc'\n",
    "            'result_ngrams' - a list of pre-processed tokens (including n-grams) of each sentence in 'doc'\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    blob = TextBlob(doc).lower()\n",
    "#     lang = blob.detect_language()\n",
    "#     print(lang)\n",
    "#     if lang != 'en':\n",
    "#         blob = blob.translate(to = 'en')\n",
    "\n",
    "    result_singles = []\n",
    "\n",
    "    tag_dict = {\"J\": 'a',  # Adjective\n",
    "                \"N\": 'n',  # Noun\n",
    "                \"V\": 'v',  # Verb\n",
    "                \"R\": 'r'}  # Adverb\n",
    "\n",
    "    # For all other types of parts of speech (including those not classified at all)\n",
    "    # the tag_dict object maps to 'None'\n",
    "    # the method w.lemmatize() defaults to 'Noun' as POS for those classified as 'None'\n",
    "\n",
    "    for sent in blob.sentences:\n",
    "\n",
    "        words_and_tags = [(w, tag_dict.get(pos[0])) for w, pos in sent.tags]\n",
    "        lemmatized_list = [w.lemmatize(tag) for w, tag in words_and_tags]\n",
    "\n",
    "        for i in range(len(lemmatized_list)):\n",
    "\n",
    "            if lemmatized_list[i] not in stop_words and len(lemmatized_list[i].lower()) > 2 and not lemmatized_list[i].isdigit():\n",
    "                result_singles.append(lemmatized_list[i].lower())\n",
    "\n",
    "    result_bigrams = ['_'.join(x) for x in ngrams(result_singles, 2)]\n",
    "\n",
    "    result_bigrams = [\n",
    "        token for token in result_bigrams if token != 'psychological_association']\n",
    "\n",
    "    result_trigrams = ['_'.join(x) for x in ngrams(result_singles, 3)]\n",
    "    result_two_plus = result_singles + result_bigrams\n",
    "    result_three_plus = result_singles + result_bigrams + result_trigrams\n",
    "\n",
    "    if n_grams == 'one':\n",
    "        result = result_singles\n",
    "    elif n_grams == 'two':\n",
    "        result = result_bigrams\n",
    "    elif n_grams == 'three':\n",
    "        result = result_trigrams\n",
    "    elif n_grams == 'two_plus':\n",
    "        result = result_two_plus\n",
    "    elif n_grams == 'three_plus':\n",
    "        result = result_three_plus\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-08T07:02:43.717002Z",
     "iopub.status.busy": "2020-05-08T07:02:43.716727Z",
     "iopub.status.idle": "2020-05-08T07:02:43.721509Z",
     "shell.execute_reply": "2020-05-08T07:02:43.720579Z",
     "shell.execute_reply.started": "2020-05-08T07:02:43.716958Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_frequency(processed_text_list):\n",
    "    \"\"\"\n",
    "    Using a built-in NLTK function that generates tuples\n",
    "    We get the frequency distribution of all words/n-grams in a tokenized list\n",
    "    We can get the proportion of the the token as a fraction of the total corpus size  ----> N/A\n",
    "    We can also sort these frequencies and proportions in descending order in a dictionary object ----> N/A\n",
    "\n",
    "    Argument(s): 'processed_text_list' - A list of pre-processed tokens\n",
    "\n",
    "    Output(s): freq_dict - A dictionary of tokens and their respective frequencies in descending order\n",
    "    \"\"\"\n",
    "    # prop_dict - A dictionary of tokens and their respective proportions as a fraction of the total corpus\n",
    "    # combined_dict - A dictionary whose values are both frequencies and proportions combined within a list\n",
    "    # \"\"\"\n",
    "\n",
    "    word_frequency = FreqDist(word for word in processed_text_list)\n",
    "\n",
    "#     sorted_counts = sorted(word_frequency.items(), key = lambda x: x[1], reverse = True)\n",
    "#     freq_dict = dict(sorted_counts)\n",
    "    freq_dict = dict(word_frequency)\n",
    "#     prop_dict = {key : freq_dict[key] * 1.0 / sum(freq_dict.values()) for key, value in freq_dict.items()}\n",
    "#     combined_dict = {key : [freq_dict[key], freq_dict[key] * 1.0 / sum(freq_dict.values())] for key, value in freq_dict.items()}\n",
    "\n",
    "    return freq_dict  # , prop_dict, combined_dict\n",
    "\n",
    "# ------------------------------------------------------------------------------------ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-08T07:12:31.648807Z",
     "iopub.status.busy": "2020-05-08T07:12:31.648547Z",
     "iopub.status.idle": "2020-05-08T07:12:31.655411Z",
     "shell.execute_reply": "2020-05-08T07:12:31.654670Z",
     "shell.execute_reply.started": "2020-05-08T07:12:31.648763Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_vocab_dictionary(vocab_column):\n",
    "    \"\"\"\n",
    "    Takes any number of token frequency dictionaries and merges them while summing \n",
    "    the respective frequencies and then calculates the proportion of the the tokens \n",
    "    as a fraction of the total corpus size and saves to text and CSV files\n",
    "\n",
    "\n",
    "    Argument(s): vocab_column - A column of dictionary objects\n",
    "\n",
    "    Output(s): merged_combined_dict - A list object containing the frequencies of all\n",
    "               merged dictionary tokens along with their respective proportions\n",
    "    \"\"\"\n",
    "\n",
    "    merged_freq_dict = {}\n",
    "    for dictionary in vocab_column:\n",
    "        for key, value in dictionary.items():  # d.items() in Python 3+\n",
    "            merged_freq_dict.setdefault(key, []).append(1)\n",
    "\n",
    "    for key, value in merged_freq_dict.items():\n",
    "        merged_freq_dict[key] = sum(value)\n",
    "\n",
    "#     total_sum = sum(merged_freq_dict.values())\n",
    "#     merged_prop_dict = {key : merged_freq_dict[key] * 1.0 / total_sum for key, value in merged_freq_dict.items()}\n",
    "#     merged_combined_dict = {key : [merged_freq_dict[key], (merged_freq_dict[key] * 1.0 / total_sum)] for key, value in merged_freq_dict.items()}\n",
    "\n",
    "    return merged_freq_dict\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------ #\n",
    "\n",
    "# ------------------------------------------------------------------------------------ #\n",
    "\n",
    "def remove_less_than(frequency_dict, less_than = 1):\n",
    "    \n",
    "    \"\"\"\n",
    "    We use the \n",
    "    \n",
    "    Argument(s): 'frequency_dict' - a dictionary \n",
    "                 'max_frequency' - \n",
    "                 'less_than' - \n",
    "    \n",
    "    Output: 'retained' - a dictionary of\n",
    "    \"\"\"\n",
    "   \n",
    "    retained_dict = {key : value for key, value in frequency_dict.items() if (value > less_than)}\n",
    "    \n",
    "    return retained_dict\n",
    "\n",
    "# ------------------------------------------------------------------------------------ #\n",
    "\n",
    "def filter_after_preprocess(processed_tokens, retained_dict):\n",
    "    \n",
    "    \"\"\"\n",
    "    We use the \n",
    "    \n",
    "    Argument(s): processed_tokens  -\n",
    "                 vocabulary_dict -\n",
    "    \n",
    "    Output: filtered \n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "\n",
    "    for token in processed_tokens:\n",
    "        if token in retained_dict.keys():\n",
    "            filtered.append(token)\n",
    "\n",
    "    return filtered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-08T07:12:32.104180Z",
     "iopub.status.busy": "2020-05-08T07:12:32.103914Z",
     "iopub.status.idle": "2020-05-08T07:12:32.111728Z",
     "shell.execute_reply": "2020-05-08T07:12:32.110955Z",
     "shell.execute_reply.started": "2020-05-08T07:12:32.104139Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_doc_term_mat(corpus_by_article, corpus_by_cluster):\n",
    "    \n",
    "    \"\"\"\n",
    "    Argument(s): corpus_by_article - (list of lists)\n",
    "                 corpus_by_cluster - flattened list \n",
    "\n",
    "    Output(s): doc_term_prob_mat, cluster_prob_mat\n",
    "    \"\"\"\n",
    "\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    cluster_count_mat = count_vectorizer.fit_transform(corpus_by_cluster)\n",
    "    doc_term_count_mat = count_vectorizer.transform(corpus_by_article)\n",
    "\n",
    "    prob_transformer = TfidfTransformer(norm='l1', use_idf=False, smooth_idf=False)\n",
    "    \n",
    "    doc_term_prob_mat = prob_transformer.fit_transform(doc_term_count_mat)\n",
    "    cluster_prob_mat = prob_transformer.fit_transform(cluster_count_mat)\n",
    "    \n",
    "    return doc_term_prob_mat, cluster_prob_mat\n",
    "\n",
    "# ------------------------------------------------------------------------------------ #\n",
    "\n",
    "def vectorize(text, corpus_by_cluster):\n",
    "    \n",
    "    count_vectorizer = CountVectorizer(lowercase=False, vocabulary=list(set(corpus_by_cluster[0].split())))\n",
    "    \n",
    "    cluster_count_mat = count_vectorizer.fit(corpus_by_cluster)\n",
    "    \n",
    "    article_count_mat = count_vectorizer.transform([' '.join(text)])\n",
    "    article_sum = sparse.diags(1/article_count_mat.sum(axis=1).A.ravel())\n",
    "    article_prob_vec = (article_sum @ article_count_mat).toarray().tolist()[0]\n",
    "    \n",
    "    return article_prob_vec\n",
    "\n",
    "# ------------------------------------------------------------------------------------ #\n",
    "\n",
    "def calculate_jsd(doc_prob_vec, cluster_prob_vec):\n",
    "    \n",
    "    jsd = distance.jensenshannon(doc_prob_vec, cluster_prob_vec)\n",
    "    \n",
    "    return jsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-08T07:18:34.735809Z",
     "iopub.status.busy": "2020-05-08T07:18:34.735557Z",
     "iopub.status.idle": "2020-05-08T07:43:41.958199Z",
     "shell.execute_reply": "2020-05-08T07:43:41.954420Z",
     "shell.execute_reply.started": "2020-05-08T07:18:34.735764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying:  now_20_ids\n",
      "Querying Cluster Number:  1\n",
      "SELECT clt.scp, tat.title, tat.abstract_text, clt.cluster FROM now_20_ids AS clt LEFT JOIN top_scp_title_concat_abstract_english AS tat ON clt.scp = tat.scp WHERE clt.cluster=1;\n",
      "The size of the cluster is:  52384\n",
      "Size after removing missing titles and abstracts:  39967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/opt/anaconda3/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6ab1cd2fc348fea24b8085543aaaf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=39967.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0238044f520444df92d33ad73bc9e47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=39967.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d0ffd1b64e4afe9760c034e320150c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=39967.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd90754ecb9a486399304a59bcf1ccb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=39967.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/swifter/swifter.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0msuppress_stdout_stderr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                 \u001b[0mtmp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m                 self._validate_apply(\n",
      "\u001b[0;32m<ipython-input-11-87a0aeed40bf>\u001b[0m in \u001b[0;36mvectorize\u001b[0;34m(text, corpus_by_cluster)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0marticle_count_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0marticle_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0marticle_count_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a05e5f650771>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mcount_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_by_cluster\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mdata_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'probability_vector'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filtered_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswifter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_by_cluster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mcluster_count_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_by_cluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mcluster_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcluster_count_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/swifter/swifter.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m    248\u001b[0m         ):  # if can't vectorize, estimate time to pandas apply\n\u001b[1;32m    249\u001b[0m             \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mtimed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_REPEATS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m             \u001b[0msample_proc_est\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimed\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN_REPEATS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mest_apply_duration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_proc_est\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SAMPLE_SIZE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/timeit.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(stmt, setup, timer, number, globals)\u001b[0m\n\u001b[1;32m    230\u001b[0m            number=default_number, globals=None):\n\u001b[1;32m    231\u001b[0m     \u001b[0;34m\"\"\"Convenience function to create Timer object and call timeit method.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m def repeat(stmt=\"pass\", setup=\"pass\", timer=default_timer,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/timeit.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/timeit.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer, _stmt)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/swifter/swifter.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0msuppress_stdout_stderr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SAMPLE_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3848\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   3831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3832\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3833\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3835\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-87a0aeed40bf>\u001b[0m in \u001b[0;36mvectorize\u001b[0;34m(text, corpus_by_cluster)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_by_cluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mcount_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_by_cluster\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mcluster_count_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_by_cluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------ #\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "with open('/Users/shreya/Documents/ernie_password.txt') as f:\n",
    "    ernie_password = f.readline()\n",
    "\n",
    "conn=psycopg2.connect(database=\"ernie\",user=\"shreya\",host=\"localhost\",password=ernie_password)\n",
    "conn.set_client_encoding('UTF8')\n",
    "conn.autocommit=True\n",
    "curs=conn.cursor()\n",
    "\n",
    "# Set schema\n",
    "schema = \"theta_plus\"\n",
    "set_schema = \"SET SEARCH_PATH TO \" + schema + \";\"   \n",
    "curs.execute(set_schema)\n",
    "\n",
    "\n",
    "# weights = ['ncf', 'now', 'sf'] \n",
    "# inflation = ['20', '30', '40', '60']\n",
    "\n",
    "# name = argv[1]\n",
    "# val = argv[2]\n",
    "# start_cluster_num = argv[3]\n",
    "\n",
    "name = 'now'\n",
    "val = '20'\n",
    "start_cluster_num = 1\n",
    "\n",
    "title_abstract_table = 'top_scp_title_concat_abstract_english'\n",
    "\n",
    "\n",
    "jsd_output = pd.DataFrame()\n",
    "\n",
    "cluster_table = name + '_' + val + '_ids'\n",
    "print(\"Querying: \", cluster_table)\n",
    "        \n",
    "        \n",
    "max_query = \"SELECT MAX(cluster) FROM \" + cluster_table + \";\"\n",
    "\n",
    "curs.execute(max_query, conn)\n",
    "max_cluster_number = curs.fetchone()[0]\n",
    "\n",
    "for cluster_num in range(start_cluster_num, 2):\n",
    "\n",
    "    print(\"Querying Cluster Number: \", str(cluster_num))\n",
    "\n",
    "\n",
    "    query = \"SELECT clt.scp, tat.title, tat.abstract_text, clt.cluster \" + \"FROM \" + cluster_table +  \" AS clt \" + \"LEFT JOIN \" + title_abstract_table + \" AS tat \" + \"ON clt.scp = tat.scp \" + \"WHERE clt.cluster=\" + str(cluster_num) + \";\"\n",
    "\n",
    "    print(query)\n",
    "\n",
    "\n",
    "    mcl_data = pd.read_sql(query, conn)\n",
    "\n",
    "    print('The size of the cluster is: ', len(mcl_data))\n",
    "    original_cluster_size = len(mcl_data)\n",
    "    mcl_data = mcl_data.dropna()\n",
    "    print('Size after removing missing titles and abstracts: ', len(mcl_data))\n",
    "    final_cluster_size = len(mcl_data)\n",
    "\n",
    "\n",
    "    if final_cluster_size < 10:\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"The cluster size is inadequate.\")\n",
    "        print(\"Moving to next cluster.\")\n",
    "        print(\"\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        data_text = mcl_data.copy()\n",
    "        data_text['scp'] = data_text.astype('str')\n",
    "        data_text['processed_title'] = data_text['title'].swifter.apply(preprocess_text)\n",
    "        data_text['processed_abstract'] = data_text['abstract_text'].swifter.apply(preprocess_text)\n",
    "        data_text['processed_all_text'] = data_text['processed_title'] + data_text['processed_abstract']\n",
    "        \n",
    "        data_text['processed_all_text_frequencies'] = data_text['processed_all_text'].swifter.apply(get_frequency)\n",
    "        data_all_text_frequency = merge_vocab_dictionary(data_text['processed_all_text_frequencies'])\n",
    "\n",
    "        retained_dict = remove_less_than(data_all_text_frequency)\n",
    "        data_text['filtered_text'] = data_text['processed_all_text'].swifter.apply(filter_after_preprocess, args = (retained_dict,))\n",
    "        \n",
    "        mcl_all_text = data_text.filtered_text.tolist()\n",
    "        corpus_by_article = [' '.join(text) for text in mcl_all_text]\n",
    "        corpus_by_cluster = [' '.join(corpus_by_article)]\n",
    "        \n",
    "        count_vectorizer = CountVectorizer(lowercase=False, vocabulary=list(set(corpus_by_cluster[0].split())))\n",
    "        data_text['probability_vector'] = data_text['filtered_text'].swifter.apply(vectorize, args=(corpus_by_cluster,))\n",
    "        cluster_count_mat = count_vectorizer.fit_transform(corpus_by_cluster)\n",
    "        cluster_sum = sparse.diags(1/cluster_count_mat.sum(axis=1).A.ravel())\n",
    "        cluster_prob_vec = (cluster_sum @ cluster_count_mat).toarray().tolist()[0]\n",
    "\n",
    "        data_text['JS_distance'] = data_text['probability_vector'].swifter.apply(calculate_jsd, args = (cluster_prob_vec,))\n",
    "\n",
    "        data_text = data_text.dropna()\n",
    "        data_text['JS_divergence'] = np.square(data_text['JS_distance'])\n",
    "        jsd_cluster_size = len(data_text)\n",
    "\n",
    "        jsd_min = min(data_text['JS_divergence'])\n",
    "        jsd_25_percentile = np.percentile(data_text['JS_divergence'], 25)\n",
    "        jsd_median = np.percentile(data_text['JS_divergence'], 50)\n",
    "        jsd_75_percentile = np.percentile(data_text['JS_divergence'], 75)\n",
    "        jsd_max = max(data_text['JS_divergence'])\n",
    "        jsd_mean = np.mean(data_text['JS_divergence'])\n",
    "        jsd_std = np.std(data_text['JS_divergence'])\n",
    "\n",
    "\n",
    "#                 print(\"\")\n",
    "#                 print('Minimum JSD value for the cluster is: ', jsd_min)\n",
    "#                 print('25th Percentile JSD value for the cluster is: ', jsd_25_percentile)\n",
    "#                 print('Median JSD value for the cluster is: ', jsd_median)\n",
    "#                 print('75th Percentile JSD value for the cluster is: ', jsd_75_percentile)\n",
    "#                 print('Maximum JSD value for the cluster is: ', jsd_max)\n",
    "#                 print('Mean JSD value for the cluster is: ', jsd_mean)\n",
    "#                 print('Std Dev of JSD for the cluster is: ', np.std(data_text['JSD']))\n",
    "#                 print(\"\")\n",
    "\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"JSD computed.\")\n",
    "        print(\"Cluster analysis complete.\")\n",
    "        print(\"\")\n",
    "        print(\"Saving to CSV\")\n",
    "        print(\"\")\n",
    "\n",
    "        result_df = pd.DataFrame({\n",
    "            'weight': name, \n",
    "            'inflation': val,\n",
    "            'cluster': cluster_num,\n",
    "            'total_size': original_cluster_size, \n",
    "            'pre_jsd_size': final_cluster_size,\n",
    "            'missing_values': (original_cluster_size-final_cluster_size,),\n",
    "            'post_jsd_size': jsd_cluster_size,\n",
    "            'jsd_nans': (final_cluster_size-jsd_cluster_size), \n",
    "            'mean_jsd': jsd_mean, \n",
    "            'min_jsd': jsd_min,\n",
    "            'percentile_25_jsd': jsd_25_percentile, \n",
    "            'median_jsd': jsd_median,\n",
    "            'percentile_75_jsd': jsd_75_percentile, \n",
    "            'max_jsd': jsd_max, \n",
    "            'std_dev_jsd': jsd_std,\n",
    "            'total_unique_bigrams': len(data_all_text_frequency),\n",
    "            'final_unique_bigrams': len(retained_dict),\n",
    "            'size_1_bigram_prop': (1-(len(retained_dict)/len(data_all_text_frequency)))})\n",
    "\n",
    "\n",
    "        jsd_output = jsd_output.append(result_df)\n",
    "#         jsd_output.to_csv(\"/Users/shreya/Documents/JSD_output.csv\", index = None, header=True, encoding='utf-8')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ALL COMPLETED.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-08T07:18:19.441529Z",
     "iopub.status.busy": "2020-05-08T07:18:19.441272Z",
     "iopub.status.idle": "2020-05-08T07:18:19.446638Z",
     "shell.execute_reply": "2020-05-08T07:18:19.445977Z",
     "shell.execute_reply.started": "2020-05-08T07:18:19.441481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsd_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
