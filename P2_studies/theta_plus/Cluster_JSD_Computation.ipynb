{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-13T18:16:03.168543Z",
     "iopub.status.busy": "2020-05-13T18:16:03.168327Z",
     "iopub.status.idle": "2020-05-13T18:16:03.174014Z",
     "shell.execute_reply": "2020-05-13T18:16:03.173431Z",
     "shell.execute_reply.started": "2020-05-13T18:16:03.168520Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shreya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import swifter  # Makes applying to datframe as fast as vectorizing\n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from textblob import TextBlob, Word\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "from sys import argv\n",
    "from scipy import sparse\n",
    "from ast import literal_eval\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "# ------------------------------------------------------------------------------------ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-13T18:04:46.718944Z",
     "iopub.status.busy": "2020-05-13T18:04:46.718730Z",
     "iopub.status.idle": "2020-05-13T18:04:46.762847Z",
     "shell.execute_reply": "2020-05-13T18:04:46.761947Z",
     "shell.execute_reply.started": "2020-05-13T18:04:46.718922Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = ['a', 'aaa', 'about', 'above', 'account', 'activity', 'addition', 'affect', \n",
    "              'after', 'again', 'against', 'age', 'ain', 'alan', 'all', 'allow', 'almost',\n",
    "              'along', 'along', 'also', 'although', 'always', 'am', 'america', 'american', \n",
    "              'among', 'amount', 'an', 'analysis', 'and', 'and/or', 'another', 'any', 'apa', \n",
    "              'appear', 'application', 'apply', 'approach', 'approximately', 'are', 'area', \n",
    "              'aren', \"aren't\", 'as', 'aspect', 'associate', 'association', 'assume', 'at', \n",
    "              'attempt', 'author', 'average', 'base', 'basis', 'be', 'because', 'become', 'been', \n",
    "              'before', 'behavior', 'being', 'below', 'best', 'between', 'both', 'but', 'by', \n",
    "              'calculate', 'calculation', 'can', 'case', 'cause', 'certain', 'change', \n",
    "              'characteristic', 'characterize', 'conclude', 'closely', 'common', 'compare', \n",
    "              'comparison', 'complete', 'complex', 'component', 'condition', 'consider', \n",
    "              'consist', 'consistent', 'constant', 'contain', 'control', 'copyright', 'could', \n",
    "              'couldn', \"couldn't\", 'd', 'data', 'database', 'day', 'decrease', 'define', \n",
    "              'demonstrate', 'depend', 'describe', 'detect', 'determine', 'develop', 'did', 'didn',\n",
    "              \"didn't\", 'differ', 'difference', 'different', 'directly', 'discuss', 'distribution', 'do', \n",
    "              'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'done', 'down', 'due', 'during', 'e.g.', \n",
    "              'each', 'early', 'effect', 'effective', 'eight', 'either', 'employ', 'enough', 'especially',\n",
    "              'establish', 'establish', 'estimate', 'etc', 'evaluate', 'even', 'evidence', 'examine', \n",
    "              'example', 'exhibit', 'exhibit', 'experiment', 'experimental', 'explain', 'factor', 'far', \n",
    "              'feature', 'few', 'find', 'finding', 'first', 'five', 'follow', 'for', 'form', 'formation', \n",
    "              'found', 'four', 'free', 'from', 'function', 'further', 'furthermore', 'general', 'give', \n",
    "              'good', 'great', 'group', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have',\n",
    "              'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'high', 'highly', \n",
    "              'him', 'himself', 'his', 'hitherto', 'how', 'however', 'human', 'i', 'if', 'important', \n",
    "              'improved', 'in', 'inc', 'include', 'increase', 'increased', 'indicate', 'individual',\n",
    "              'induce', 'influence', 'information', 'initial', 'interaction', 'into', 'is', \n",
    "              'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'kg', 'km', 'know', 'large', 'latter', \n",
    "              'least', 'less', 'level', 'likely', 'limit', 'line', 'll', 'low', 'm', 'ma', 'made', \n",
    "              'mainly', 'make', 'man', 'many', 'maximum', 'may', 'me', 'mean', 'measure', \n",
    "              'measurement', 'method', 'mg', 'might', 'mightn', \"mightn't\", 'ml', 'mm', 'monkey', \n",
    "              'month', 'more', 'most', 'mostly', 'mouse', 'much', 'multiple', 'must', 'mustn',\n",
    "              \"mustn't\", 'my', 'myself', 'near', 'nearly', 'need', 'needn', \"needn't\", 'neither',\n",
    "              'new', 'nih', 'nine', 'no', 'nor', 'normal', 'not', 'noticeable', 'now', 'number', \n",
    "              'o', 'observation', 'observe', 'obtain', 'obtained', 'occur', 'of', 'off', 'often', \n",
    "              'on', 'once', 'one', 'only', 'or', 'order', 'other', 'our', 'ours', 'ourselves', \n",
    "              'out', 'over', 'overall', 'own', 'parameter', 'part', 'particular', 'per', 'perform', \n",
    "              'perhaps', 'period', 'phase', 'physical', 'pmid', 'point', 'population', 'possible', \n",
    "              'potential', 'presence', 'present', 'press', 'previous', 'previously', 'probably', \n",
    "              'process', 'produce', 'product', 'property', 'propose', 'provide', 'psycinfo', \n",
    "              'publish', 'quantity', 'quite', 'rabbit', 'range', 'rat', 'rate', 'rather', 're', \n",
    "              'reaction', 'really', 'receive', 'recent', 'recently', 'record', 'ref', 'reference', \n",
    "              'regarding', 'relate', 'relation', 'relationship', 'relative', 'relatively', 'relevant', \n",
    "              'remain', 'report', 'represent', 'require', 'respectively', 'response', 'result', \n",
    "              'reveal', 'review', 'right', 'right reserved', 'role', 's', 'same', 'sample', 'second',\n",
    "              'see', 'seem', 'seen', 'separate', 'set', 'seven', 'several', 'shan', \"shan't\", 'she', \n",
    "              \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'show', 'showed', 'shown', \n",
    "              'shows', 'significant', 'significantly', 'similar', 'simple', 'since', 'six', 'size',\n",
    "              'small', 'so', 'society', 'some', 'specific', 'state', 'strong', 'study', 'subject',\n",
    "              'subsequent', 'such', 'suggest', 'support', 't', 'take', 'task', 'technique', 'ten', \n",
    "              'term', 'test', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them',\n",
    "              'themselves', 'then', 'theory', 'there', 'therefore', 'these', 'they', 'this', 'those',\n",
    "              'three', 'through', 'thus', 'time', 'to', 'together', 'too', 'total', 'two', 'type',\n",
    "              'under', 'university', 'unknown', 'unlikely', 'until', 'up', 'upon', 'use', 'used', \n",
    "              'useful', 'using', 'usually', 'value', 'various', 'vary', 've', 'very', 'via', 'view', \n",
    "              'was', 'wasn', \"wasn't\", 'we', 'well', 'were', 'weren', \"weren't\", 'what', 'whatever', \n",
    "              'when', 'where', 'whereas', 'whether', 'which', 'while', 'who', 'whom', 'why', 'wiley', \n",
    "              'will', 'with', 'within', 'without', 'won', \"won't\", 'work', 'would', 'wouldn', \n",
    "              \"wouldn't\", 'y', 'yield', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours',\n",
    "              'yourself', 'yourselves', 'involve', 'investigate', 'apparent', 'identify', 'assess']\n",
    "\n",
    "\n",
    "def preprocess_text(doc, n_grams='one'):\n",
    "    \"\"\"\n",
    "    Pre-processing using TextBlob: \n",
    "    tokenizing, converting to lower-case, and lemmatization based on POS tagging, \n",
    "    removing stop-words, and retaining tokens greater than length 2\n",
    "\n",
    "    We can also choose to include n_grams (n = 1,2,3) in the final output\n",
    "\n",
    "    Argument(s): 'doc' - a string of words or sentences.\n",
    "                 'n_grams' - one: only unigrams (tokens consisting of one word each)\n",
    "                           - two: only bigrams\n",
    "                           - two_plus: unigrams + bigrams\n",
    "                           - three: only trigrams \n",
    "                           - three_plus: unigrams + bigrams + trigrams\n",
    "\n",
    "    Output: 'reuslt_singles' - a list of pre-processed tokens (individual words) of each sentence in 'doc'\n",
    "            'result_ngrams' - a list of pre-processed tokens (including n-grams) of each sentence in 'doc'\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    blob = TextBlob(doc).lower()\n",
    "#     lang = blob.detect_language()\n",
    "#     print(lang)\n",
    "#     if lang != 'en':\n",
    "#         blob = blob.translate(to = 'en')\n",
    "\n",
    "    result_singles = []\n",
    "\n",
    "    tag_dict = {\"J\": 'a',  # Adjective\n",
    "                \"N\": 'n',  # Noun\n",
    "                \"V\": 'v',  # Verb\n",
    "                \"R\": 'r'}  # Adverb\n",
    "\n",
    "    # For all other types of parts of speech (including those not classified at all)\n",
    "    # the tag_dict object maps to 'None'\n",
    "    # the method w.lemmatize() defaults to 'Noun' as POS for those classified as 'None'\n",
    "\n",
    "    for sent in blob.sentences:\n",
    "\n",
    "        words_and_tags = [(w, tag_dict.get(pos[0])) for w, pos in sent.tags]\n",
    "        lemmatized_list = [w.lemmatize(tag) for w, tag in words_and_tags]\n",
    "\n",
    "        for i in range(len(lemmatized_list)):\n",
    "\n",
    "            if lemmatized_list[i] not in stop_words and len(lemmatized_list[i].lower()) > 2 and not lemmatized_list[i].isdigit():\n",
    "                result_singles.append(lemmatized_list[i].lower())\n",
    "\n",
    "    result_bigrams = ['_'.join(x) for x in ngrams(result_singles, 2)]\n",
    "\n",
    "    result_bigrams = [\n",
    "        token for token in result_bigrams if token != 'psychological_association']\n",
    "\n",
    "    result_trigrams = ['_'.join(x) for x in ngrams(result_singles, 3)]\n",
    "    result_two_plus = result_singles + result_bigrams\n",
    "    result_three_plus = result_singles + result_bigrams + result_trigrams\n",
    "\n",
    "    if n_grams == 'one':\n",
    "        result = result_singles\n",
    "    elif n_grams == 'two':\n",
    "        result = result_bigrams\n",
    "    elif n_grams == 'three':\n",
    "        result = result_trigrams\n",
    "    elif n_grams == 'two_plus':\n",
    "        result = result_two_plus\n",
    "    elif n_grams == 'three_plus':\n",
    "        result = result_three_plus\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-13T18:04:47.005676Z",
     "iopub.status.busy": "2020-05-13T18:04:47.005339Z",
     "iopub.status.idle": "2020-05-13T18:04:47.009888Z",
     "shell.execute_reply": "2020-05-13T18:04:47.009194Z",
     "shell.execute_reply.started": "2020-05-13T18:04:47.005639Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_frequency(processed_text_list):\n",
    "    \"\"\"\n",
    "    Using a built-in NLTK function that generates tuples\n",
    "    We get the frequency distribution of all words/n-grams in a tokenized list\n",
    "    We can get the proportion of the the token as a fraction of the total corpus size  ----> N/A\n",
    "    We can also sort these frequencies and proportions in descending order in a dictionary object ----> N/A\n",
    "\n",
    "    Argument(s): 'processed_text_list' - A list of pre-processed tokens\n",
    "\n",
    "    Output(s): freq_dict - A dictionary of tokens and their respective frequencies in descending order\n",
    "    \"\"\"\n",
    "    # prop_dict - A dictionary of tokens and their respective proportions as a fraction of the total corpus\n",
    "    # combined_dict - A dictionary whose values are both frequencies and proportions combined within a list\n",
    "    # \"\"\"\n",
    "\n",
    "    word_frequency = FreqDist(word for word in processed_text_list)\n",
    "\n",
    "#     sorted_counts = sorted(word_frequency.items(), key = lambda x: x[1], reverse = True)\n",
    "#     freq_dict = dict(sorted_counts)\n",
    "    freq_dict = dict(word_frequency)\n",
    "#     prop_dict = {key : freq_dict[key] * 1.0 / sum(freq_dict.values()) for key, value in freq_dict.items()}\n",
    "#     combined_dict = {key : [freq_dict[key], freq_dict[key] * 1.0 / sum(freq_dict.values())] for key, value in freq_dict.items()}\n",
    "\n",
    "    return freq_dict  # , prop_dict, combined_dict\n",
    "\n",
    "# ------------------------------------------------------------------------------------ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-13T18:04:47.318340Z",
     "iopub.status.busy": "2020-05-13T18:04:47.318123Z",
     "iopub.status.idle": "2020-05-13T18:04:47.326186Z",
     "shell.execute_reply": "2020-05-13T18:04:47.325195Z",
     "shell.execute_reply.started": "2020-05-13T18:04:47.318318Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_vocab_dictionary(vocab_column):\n",
    "    \"\"\"\n",
    "    Takes any number of token frequency dictionaries and merges them while summing \n",
    "    the respective frequencies and then calculates the proportion of the the tokens \n",
    "    as a fraction of the total corpus size and saves to text and CSV files\n",
    "\n",
    "\n",
    "    Argument(s): vocab_column - A column of dictionary objects\n",
    "\n",
    "    Output(s): merged_combined_dict - A list object containing the frequencies of all\n",
    "               merged dictionary tokens along with their respective proportions\n",
    "    \"\"\"\n",
    "\n",
    "    merged_freq_dict = {}\n",
    "    for dictionary in vocab_column:\n",
    "        for key, value in dictionary.items():  # d.items() in Python 3+\n",
    "            merged_freq_dict.setdefault(key, []).append(1)\n",
    "\n",
    "    for key, value in merged_freq_dict.items():\n",
    "        merged_freq_dict[key] = sum(value)\n",
    "\n",
    "#     total_sum = sum(merged_freq_dict.values())\n",
    "#     merged_prop_dict = {key : merged_freq_dict[key] * 1.0 / total_sum for key, value in merged_freq_dict.items()}\n",
    "#     merged_combined_dict = {key : [merged_freq_dict[key], (merged_freq_dict[key] * 1.0 / total_sum)] for key, value in merged_freq_dict.items()}\n",
    "\n",
    "    return merged_freq_dict\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------ #\n",
    "\n",
    "# ------------------------------------------------------------------------------------ #\n",
    "\n",
    "def remove_less_than(frequency_dict, less_than = 1):\n",
    "    \n",
    "    \"\"\"\n",
    "    We use the \n",
    "    \n",
    "    Argument(s): 'frequency_dict' - a dictionary \n",
    "                 'max_frequency' - \n",
    "                 'less_than' - \n",
    "    \n",
    "    Output: 'retained' - a dictionary of\n",
    "    \"\"\"\n",
    "   \n",
    "    retained_dict = {key : value for key, value in frequency_dict.items() if (value > less_than)}\n",
    "    \n",
    "    return retained_dict\n",
    "\n",
    "# ------------------------------------------------------------------------------------ #\n",
    "\n",
    "def filter_after_preprocess(processed_tokens, retained_dict):\n",
    "    \n",
    "    \"\"\"\n",
    "    We use the \n",
    "    \n",
    "    Argument(s): processed_tokens  -\n",
    "                 vocabulary_dict -\n",
    "    \n",
    "    Output: filtered \n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "\n",
    "    for token in processed_tokens:\n",
    "        if token in retained_dict.keys():\n",
    "            filtered.append(token)\n",
    "\n",
    "    return filtered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-13T18:04:48.021131Z",
     "iopub.status.busy": "2020-05-13T18:04:48.020919Z",
     "iopub.status.idle": "2020-05-13T18:04:48.026082Z",
     "shell.execute_reply": "2020-05-13T18:04:48.025463Z",
     "shell.execute_reply.started": "2020-05-13T18:04:48.021107Z"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------ #\n",
    "\n",
    "def vectorize(text, corpus_by_cluster, count_vectorizer):\n",
    "    \n",
    "    article_count_mat = count_vectorizer.transform([' '.join(text)])\n",
    "    article_sum = sparse.diags(1/article_count_mat.sum(axis=1).A.ravel())\n",
    "    article_prob_vec = (article_sum @ article_count_mat).toarray()\n",
    "    \n",
    "    return article_prob_vec\n",
    "\n",
    "# # ------------------------------------------------------------------------------------ #\n",
    "# from scipy.special import rel_entr\n",
    "\n",
    "def calculate_jsd(doc_prob_vec, cluster_prob_vec, base=None):\n",
    "    \n",
    "    jsd = distance.jensenshannon(doc_prob_vec.tolist()[0], cluster_prob_vec)\n",
    "\n",
    "    return jsd\n",
    "\n",
    "\n",
    "def fix_eval_issue(doc):\n",
    "    return literal_eval(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsd_output = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-13T18:19:35.249592Z",
     "iopub.status.busy": "2020-05-13T18:19:35.249328Z",
     "iopub.status.idle": "2020-05-13T18:19:35.536437Z",
     "shell.execute_reply": "2020-05-13T18:19:35.535426Z",
     "shell.execute_reply.started": "2020-05-13T18:19:35.249564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying:  sf_20_ids\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "with open('/Users/shreya/Documents/ernie_password.txt') as f:\n",
    "    ernie_password = f.readline()\n",
    "\n",
    "conn=psycopg2.connect(database=\"ernie\",user=\"shreya\",host=\"localhost\",password=ernie_password)\n",
    "conn.set_client_encoding('UTF8')\n",
    "conn.autocommit=True\n",
    "curs=conn.cursor()\n",
    "\n",
    "# Set schema\n",
    "schema = \"theta_plus\"\n",
    "set_schema = \"SET SEARCH_PATH TO \" + schema + \";\"   \n",
    "curs.execute(set_schema)\n",
    "\n",
    "\n",
    "# weights = ['ncf', 'now', 'sf'] \n",
    "# inflation = ['20', '30', '40', '60']\n",
    "\n",
    "# name = argv[1]\n",
    "# val = argv[2]\n",
    "# start_cluster_num = argv[3]\n",
    "\n",
    "name = 'sf'\n",
    "val = 20\n",
    "start_cluster_num = 1\n",
    "\n",
    "\n",
    "        \n",
    "title_abstract_table = 'top_scp_title_concat_abstract_english'\n",
    "\n",
    "\n",
    "cluster_table = name + '_' + str(val) + '_ids'\n",
    "print(\"Querying: \", cluster_table)\n",
    "\n",
    "\n",
    "max_query = \"SELECT MAX(cluster) FROM \" + cluster_table + \";\"\n",
    "\n",
    "curs.execute(max_query, conn)\n",
    "max_cluster_number = curs.fetchone()[0]\n",
    "\n",
    "# for cluster_num in range(int(start_cluster_num), max_cluster_number+1):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-15T20:48:15.860197Z",
     "iopub.status.busy": "2020-05-15T20:48:15.852949Z",
     "iopub.status.idle": "2020-05-15T20:48:16.239557Z",
     "shell.execute_reply": "2020-05-15T20:48:16.238067Z",
     "shell.execute_reply.started": "2020-05-15T20:48:15.859171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying Cluster Number:  10\n",
      "SELECT tat.*, clt.cluster FROM sf_20_ids AS clt LEFT JOIN top_scp_title_concat_abstract_english AS tat ON clt.scp = tat.scp WHERE clt.cluster=10;\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql: SELECT tat.*, clt.cluster FROM sf_20_ids AS clt LEFT JOIN top_scp_title_concat_abstract_english AS tat ON clt.scp = tat.scp WHERE clt.cluster=10;\nserver closed the connection unexpectedly\n\tThis probably means the server terminated abnormally\n\tbefore or while processing the request.\n\nunable to rollback",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1586\u001b[0;31m             \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1587\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: server closed the connection unexpectedly\n\tThis probably means the server terminated abnormally\n\tbefore or while processing the request.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInterfaceError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1590\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1591\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minner_exc\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInterfaceError\u001b[0m: connection already closed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-9169e975a0c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mmcl_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The size of the cluster is: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmcl_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m         )\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_query\u001b[0;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m   1631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1633\u001b[0;31m         \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1634\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1593\u001b[0m                     \u001b[0;34mf\"Execution failed on sql: {args[0]}\\n{exc}\\nunable to rollback\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m                 )\n\u001b[0;32m-> 1595\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0minner_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m             \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatabaseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Execution failed on sql '{args[0]}': {exc}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql: SELECT tat.*, clt.cluster FROM sf_20_ids AS clt LEFT JOIN top_scp_title_concat_abstract_english AS tat ON clt.scp = tat.scp WHERE clt.cluster=10;\nserver closed the connection unexpectedly\n\tThis probably means the server terminated abnormally\n\tbefore or while processing the request.\n\nunable to rollback"
     ]
    }
   ],
   "source": [
    "for cluster_num in range(10,12):\n",
    "    \n",
    "    start_time=time.time()\n",
    "            \n",
    "    print(\"Querying Cluster Number: \", str(cluster_num))\n",
    "\n",
    "\n",
    "    query = \"SELECT tat.*, clt.cluster \" + \"FROM \" + cluster_table +  \" AS clt \" + \"LEFT JOIN \" + title_abstract_table + \" AS tat \" + \"ON clt.scp = tat.scp \" + \"WHERE clt.cluster=\" + str(cluster_num) + \";\"\n",
    "\n",
    "    print(query)\n",
    "\n",
    "\n",
    "    mcl_data = pd.read_sql(query, conn)\n",
    "\n",
    "    print('The size of the cluster is: ', len(mcl_data))\n",
    "    original_cluster_size = len(mcl_data)\n",
    "    mcl_data = mcl_data.dropna()\n",
    "    print('Size after removing missing titles and abstracts: ', len(mcl_data))\n",
    "    final_cluster_size = len(mcl_data)\n",
    "\n",
    "\n",
    "    if final_cluster_size < 10:\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"The cluster size is inadequate.\")\n",
    "        print(\"Moving to next cluster.\")\n",
    "        print(\"\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        data_text = mcl_data.copy()\n",
    "        data_text['scp'] = data_text.astype('str')\n",
    "\n",
    "        data_text['all_text'] = data_text['title'] + data_text['abstract_text']\n",
    "        print(\"\")\n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "\n",
    "#         p = mp.Pool(mp.cpu_count())\n",
    "#         data_text['processed_all_text'] = p.map(preprocess_text, data_text['all_text'])\n",
    "#         p.close()\n",
    "\n",
    "        data_text['processed_all_text'] = data_text['all_text'].swifter.apply(preprocess_text)\n",
    "\n",
    "        print(\"Done with preprocessing.\")\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        data_text['processed_all_text_frequencies'] = data_text['processed_all_text'].swifter.apply(get_frequency)\n",
    "        data_all_text_frequency = merge_vocab_dictionary(data_text['processed_all_text_frequencies'])\n",
    "        retained_dict = remove_less_than(data_all_text_frequency)\n",
    "        data_text['filtered_text'] = data_text['processed_all_text'].swifter.apply(filter_after_preprocess, args = (retained_dict,))\n",
    "\n",
    "        mcl_all_text = data_text.filtered_text.tolist()\n",
    "        corpus_by_article = [' '.join(text) for text in mcl_all_text]\n",
    "        corpus_by_cluster = [' '.join(corpus_by_article)]\n",
    "\n",
    "        count_vectorizer = CountVectorizer(lowercase=False, vocabulary=list(set(corpus_by_cluster[0].split())))\n",
    "        cluster_count_mat = count_vectorizer.fit_transform(corpus_by_cluster)\n",
    "\n",
    "        data_text['probability_vector'] = data_text['processed_all_text'].swifter.apply(vectorize, args=(corpus_by_cluster, count_vectorizer,))\n",
    "\n",
    "        cluster_sum = sparse.diags(1/cluster_count_mat.sum(axis=1).A.ravel())\n",
    "        cluster_prob_vec = (cluster_sum @ cluster_count_mat).toarray().tolist()[0]\n",
    "\n",
    "        data_text['JS_distance'] = data_text['probability_vector'].swifter.apply(calculate_jsd, args = (cluster_prob_vec,))\n",
    "\n",
    "        data_text = data_text.dropna()\n",
    "\n",
    "        data_text['JS_divergence'] = np.square(data_text['JS_distance'])\n",
    "        jsd_cluster_size = len(data_text)\n",
    "\n",
    "        jsd_min = min(data_text['JS_divergence'])\n",
    "        jsd_25_percentile = np.percentile(data_text['JS_divergence'], 25)\n",
    "        jsd_median = np.percentile(data_text['JS_divergence'], 50)\n",
    "        jsd_75_percentile = np.percentile(data_text['JS_divergence'], 75)\n",
    "        jsd_max = max(data_text['JS_divergence'])\n",
    "        jsd_mean = np.mean(data_text['JS_divergence'])\n",
    "        jsd_std = np.std(data_text['JS_divergence'])\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"JSD computed.\")\n",
    "        print(\"Cluster analysis complete.\")\n",
    "        print(\"\")\n",
    "        print(\"Saving to CSV\")\n",
    "        print(\"\")\n",
    "\n",
    "        result_df = pd.DataFrame({\n",
    "            'weight': name, \n",
    "            'inflation': val,\n",
    "            'cluster': cluster_num,\n",
    "            'total_size': original_cluster_size, \n",
    "            'pre_jsd_size': final_cluster_size,\n",
    "            'missing_values': (original_cluster_size-final_cluster_size,),\n",
    "            'post_jsd_size': jsd_cluster_size,\n",
    "            'jsd_nans': (final_cluster_size-jsd_cluster_size), \n",
    "            'mean_jsd': jsd_mean, \n",
    "            'min_jsd': jsd_min,\n",
    "            'percentile_25_jsd': jsd_25_percentile, \n",
    "            'median_jsd': jsd_median,\n",
    "            'percentile_75_jsd': jsd_75_percentile, \n",
    "            'max_jsd': jsd_max, \n",
    "            'std_dev_jsd': jsd_std,\n",
    "            'total_unique_unigrams': len(data_all_text_frequency),\n",
    "            'final_unique_unigrams': len(retained_dict),\n",
    "            'size_1_unigram_prop': (1-(len(retained_dict)/len(data_all_text_frequency)))})\n",
    "        \n",
    "        jsd_output.append(result_df)\n",
    "\n",
    "#         result_df.to_csv(\"/home/shreya/mcl_jsd/JSD_output_unigrams.csv\", mode = 'a', index = None, header=False, encoding='utf-8')\n",
    "        \n",
    "        print(f'Time taken for cluster {cluster_num} in {name + \"_\" + str(val)} is {time.time()-start_time} seconds.')\n",
    "        print(\"\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-13T18:24:43.626858Z",
     "iopub.status.busy": "2020-05-13T18:24:43.626628Z",
     "iopub.status.idle": "2020-05-13T18:24:43.643040Z",
     "shell.execute_reply": "2020-05-13T18:24:43.642275Z",
     "shell.execute_reply.started": "2020-05-13T18:24:43.626821Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "      <th>inflation</th>\n",
       "      <th>cluster</th>\n",
       "      <th>total_size</th>\n",
       "      <th>pre_jsd_size</th>\n",
       "      <th>missing_values</th>\n",
       "      <th>post_jsd_size</th>\n",
       "      <th>jsd_nans</th>\n",
       "      <th>mean_jsd</th>\n",
       "      <th>min_jsd</th>\n",
       "      <th>percentile_25_jsd</th>\n",
       "      <th>median_jsd</th>\n",
       "      <th>percentile_75_jsd</th>\n",
       "      <th>max_jsd</th>\n",
       "      <th>std_dev_jsd</th>\n",
       "      <th>total_unique_bigrams</th>\n",
       "      <th>final_unique_bigrams</th>\n",
       "      <th>size_1_bigram_prop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ncf</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>209</td>\n",
       "      <td>171</td>\n",
       "      <td>38</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>0.537081</td>\n",
       "      <td>0.431368</td>\n",
       "      <td>0.507440</td>\n",
       "      <td>0.535333</td>\n",
       "      <td>0.563229</td>\n",
       "      <td>0.652737</td>\n",
       "      <td>0.044371</td>\n",
       "      <td>2800</td>\n",
       "      <td>1130</td>\n",
       "      <td>0.596429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ncf</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>172</td>\n",
       "      <td>129</td>\n",
       "      <td>43</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>0.507042</td>\n",
       "      <td>0.365967</td>\n",
       "      <td>0.465629</td>\n",
       "      <td>0.504437</td>\n",
       "      <td>0.540049</td>\n",
       "      <td>0.666213</td>\n",
       "      <td>0.055103</td>\n",
       "      <td>3211</td>\n",
       "      <td>1286</td>\n",
       "      <td>0.599502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sf</td>\n",
       "      <td>20</td>\n",
       "      <td>33</td>\n",
       "      <td>352</td>\n",
       "      <td>266</td>\n",
       "      <td>86</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "      <td>0.544540</td>\n",
       "      <td>0.454334</td>\n",
       "      <td>0.511597</td>\n",
       "      <td>0.541845</td>\n",
       "      <td>0.574335</td>\n",
       "      <td>0.648937</td>\n",
       "      <td>0.041672</td>\n",
       "      <td>4812</td>\n",
       "      <td>1809</td>\n",
       "      <td>0.624065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sf</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>1735</td>\n",
       "      <td>1464</td>\n",
       "      <td>271</td>\n",
       "      <td>1464</td>\n",
       "      <td>0</td>\n",
       "      <td>0.580806</td>\n",
       "      <td>0.479322</td>\n",
       "      <td>0.556346</td>\n",
       "      <td>0.580577</td>\n",
       "      <td>0.605896</td>\n",
       "      <td>0.684941</td>\n",
       "      <td>0.036219</td>\n",
       "      <td>13555</td>\n",
       "      <td>4466</td>\n",
       "      <td>0.670527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sf</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>24615</td>\n",
       "      <td>16963</td>\n",
       "      <td>7652</td>\n",
       "      <td>16963</td>\n",
       "      <td>0</td>\n",
       "      <td>0.622347</td>\n",
       "      <td>0.506504</td>\n",
       "      <td>0.605088</td>\n",
       "      <td>0.622785</td>\n",
       "      <td>0.640246</td>\n",
       "      <td>0.690453</td>\n",
       "      <td>0.025280</td>\n",
       "      <td>103662</td>\n",
       "      <td>32765</td>\n",
       "      <td>0.683925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sf</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>674</td>\n",
       "      <td>568</td>\n",
       "      <td>106</td>\n",
       "      <td>568</td>\n",
       "      <td>0</td>\n",
       "      <td>0.527106</td>\n",
       "      <td>0.415535</td>\n",
       "      <td>0.495469</td>\n",
       "      <td>0.523509</td>\n",
       "      <td>0.558557</td>\n",
       "      <td>0.665511</td>\n",
       "      <td>0.046411</td>\n",
       "      <td>8578</td>\n",
       "      <td>3240</td>\n",
       "      <td>0.622290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sf</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>666</td>\n",
       "      <td>621</td>\n",
       "      <td>45</td>\n",
       "      <td>621</td>\n",
       "      <td>0</td>\n",
       "      <td>0.562951</td>\n",
       "      <td>0.445049</td>\n",
       "      <td>0.541041</td>\n",
       "      <td>0.565048</td>\n",
       "      <td>0.585779</td>\n",
       "      <td>0.657189</td>\n",
       "      <td>0.034632</td>\n",
       "      <td>6227</td>\n",
       "      <td>2385</td>\n",
       "      <td>0.616991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  weight  inflation  cluster  total_size  pre_jsd_size  missing_values  \\\n",
       "0    ncf         20        2         209           171              38   \n",
       "0    ncf         20        3         172           129              43   \n",
       "0     sf         20       33         352           266              86   \n",
       "0     sf         20        3        1735          1464             271   \n",
       "0     sf         20        1       24615         16963            7652   \n",
       "0     sf         20       10         674           568             106   \n",
       "0     sf         20       11         666           621              45   \n",
       "\n",
       "   post_jsd_size  jsd_nans  mean_jsd   min_jsd  percentile_25_jsd  median_jsd  \\\n",
       "0            171         0  0.537081  0.431368           0.507440    0.535333   \n",
       "0            129         0  0.507042  0.365967           0.465629    0.504437   \n",
       "0            266         0  0.544540  0.454334           0.511597    0.541845   \n",
       "0           1464         0  0.580806  0.479322           0.556346    0.580577   \n",
       "0          16963         0  0.622347  0.506504           0.605088    0.622785   \n",
       "0            568         0  0.527106  0.415535           0.495469    0.523509   \n",
       "0            621         0  0.562951  0.445049           0.541041    0.565048   \n",
       "\n",
       "   percentile_75_jsd   max_jsd  std_dev_jsd  total_unique_bigrams  \\\n",
       "0           0.563229  0.652737     0.044371                  2800   \n",
       "0           0.540049  0.666213     0.055103                  3211   \n",
       "0           0.574335  0.648937     0.041672                  4812   \n",
       "0           0.605896  0.684941     0.036219                 13555   \n",
       "0           0.640246  0.690453     0.025280                103662   \n",
       "0           0.558557  0.665511     0.046411                  8578   \n",
       "0           0.585779  0.657189     0.034632                  6227   \n",
       "\n",
       "   final_unique_bigrams  size_1_bigram_prop  \n",
       "0                  1130            0.596429  \n",
       "0                  1286            0.599502  \n",
       "0                  1809            0.624065  \n",
       "0                  4466            0.670527  \n",
       "0                 32765            0.683925  \n",
       "0                  3240            0.622290  \n",
       "0                  2385            0.616991  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsd_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
