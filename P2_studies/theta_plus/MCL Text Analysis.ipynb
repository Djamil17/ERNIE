{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-19T13:33:41.987438Z",
     "iopub.status.busy": "2020-05-19T13:33:41.987220Z",
     "iopub.status.idle": "2020-05-19T13:33:41.999017Z",
     "shell.execute_reply": "2020-05-19T13:33:41.998502Z",
     "shell.execute_reply.started": "2020-05-19T13:33:41.987417Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shreya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "import pyLDAvis\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from scipy.spatial import distance\n",
    "import collections\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import swifter  # Makes applying to datframe as fast as vectorizing\n",
    "from gensim import models  # For TF-IDF, LDA\n",
    "import gensim\n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from pandas import Series, DataFrame\n",
    "from textblob import TextBlob, Word\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "from scipy import sparse\n",
    "from ast import literal_eval\n",
    "# Visualization\n",
    "plt.style.use('seaborn-paper')\n",
    "%matplotlib inline\n",
    "\n",
    "# LDA Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-19T13:31:11.070360Z",
     "iopub.status.busy": "2020-05-19T13:31:11.070147Z",
     "iopub.status.idle": "2020-05-19T13:31:11.097308Z",
     "shell.execute_reply": "2020-05-19T13:31:11.096337Z",
     "shell.execute_reply.started": "2020-05-19T13:31:11.070338Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = ['a', 'aaa', 'about', 'above', 'account', 'activity', 'addition', 'affect', \n",
    "              'after', 'again', 'against', 'age', 'ain', 'alan', 'all', 'allow', 'almost',\n",
    "              'along', 'along', 'also', 'although', 'always', 'am', 'america', 'american', \n",
    "              'among', 'amount', 'an', 'analysis', 'and', 'and/or', 'another', 'any', 'apa', \n",
    "              'appear', 'application', 'apply', 'approach', 'approximately', 'are', 'area', \n",
    "              'aren', \"aren't\", 'as', 'aspect', 'associate', 'association', 'assume', 'at', \n",
    "              'attempt', 'author', 'average', 'base', 'basis', 'be', 'because', 'become', 'been', \n",
    "              'before', 'behavior', 'being', 'below', 'best', 'between', 'both', 'but', 'by', \n",
    "              'calculate', 'calculation', 'can', 'case', 'cause', 'certain', 'change', \n",
    "              'characteristic', 'characterize', 'conclude', 'closely', 'common', 'compare', \n",
    "              'comparison', 'complete', 'complex', 'component', 'condition', 'consider', \n",
    "              'consist', 'consistent', 'constant', 'contain', 'control', 'copyright', 'could', \n",
    "              'couldn', \"couldn't\", 'd', 'data', 'database', 'day', 'decrease', 'define', \n",
    "              'demonstrate', 'depend', 'describe', 'detect', 'determine', 'develop', 'did', 'didn',\n",
    "              \"didn't\", 'differ', 'difference', 'different', 'directly', 'discuss', 'distribution', 'do', \n",
    "              'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'done', 'down', 'due', 'during', 'e.g.', \n",
    "              'each', 'early', 'effect', 'effective', 'eight', 'either', 'employ', 'enough', 'especially',\n",
    "              'establish', 'establish', 'estimate', 'etc', 'evaluate', 'even', 'evidence', 'examine', \n",
    "              'example', 'exhibit', 'exhibit', 'experiment', 'experimental', 'explain', 'factor', 'far', \n",
    "              'feature', 'few', 'find', 'finding', 'first', 'five', 'follow', 'for', 'form', 'formation', \n",
    "              'found', 'four', 'free', 'from', 'function', 'further', 'furthermore', 'general', 'give', \n",
    "              'good', 'great', 'group', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have',\n",
    "              'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'high', 'highly', \n",
    "              'him', 'himself', 'his', 'hitherto', 'how', 'however', 'human', 'i', 'if', 'important', \n",
    "              'improved', 'in', 'inc', 'include', 'increase', 'increased', 'indicate', 'individual',\n",
    "              'induce', 'influence', 'information', 'initial', 'interaction', 'into', 'is', \n",
    "              'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'kg', 'km', 'know', 'large', 'latter', \n",
    "              'least', 'less', 'level', 'likely', 'limit', 'line', 'll', 'low', 'm', 'ma', 'made', \n",
    "              'mainly', 'make', 'man', 'many', 'maximum', 'may', 'me', 'mean', 'measure', \n",
    "              'measurement', 'method', 'mg', 'might', 'mightn', \"mightn't\", 'ml', 'mm', 'monkey', \n",
    "              'month', 'more', 'most', 'mostly', 'mouse', 'much', 'multiple', 'must', 'mustn',\n",
    "              \"mustn't\", 'my', 'myself', 'near', 'nearly', 'need', 'needn', \"needn't\", 'neither',\n",
    "              'new', 'nih', 'nine', 'no', 'nor', 'normal', 'not', 'noticeable', 'now', 'number', \n",
    "              'o', 'observation', 'observe', 'obtain', 'obtained', 'occur', 'of', 'off', 'often', \n",
    "              'on', 'once', 'one', 'only', 'or', 'order', 'other', 'our', 'ours', 'ourselves', \n",
    "              'out', 'over', 'overall', 'own', 'parameter', 'part', 'particular', 'per', 'perform', \n",
    "              'perhaps', 'period', 'phase', 'physical', 'pmid', 'point', 'population', 'possible', \n",
    "              'potential', 'presence', 'present', 'press', 'previous', 'previously', 'probably', \n",
    "              'process', 'produce', 'product', 'property', 'propose', 'provide', 'psycinfo', \n",
    "              'publish', 'quantity', 'quite', 'rabbit', 'range', 'rat', 'rate', 'rather', 're', \n",
    "              'reaction', 'really', 'receive', 'recent', 'recently', 'record', 'ref', 'reference', \n",
    "              'regarding', 'relate', 'relation', 'relationship', 'relative', 'relatively', 'relevant', \n",
    "              'remain', 'report', 'represent', 'require', 'respectively', 'response', 'result', \n",
    "              'reveal', 'review', 'right', 'right reserved', 'role', 's', 'same', 'sample', 'second',\n",
    "              'see', 'seem', 'seen', 'separate', 'set', 'seven', 'several', 'shan', \"shan't\", 'she', \n",
    "              \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'show', 'showed', 'shown', \n",
    "              'shows', 'significant', 'significantly', 'similar', 'simple', 'since', 'six', 'size',\n",
    "              'small', 'so', 'society', 'some', 'specific', 'state', 'strong', 'study', 'subject',\n",
    "              'subsequent', 'such', 'suggest', 'support', 't', 'take', 'task', 'technique', 'ten', \n",
    "              'term', 'test', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them',\n",
    "              'themselves', 'then', 'theory', 'there', 'therefore', 'these', 'they', 'this', 'those',\n",
    "              'three', 'through', 'thus', 'time', 'to', 'together', 'too', 'total', 'two', 'type',\n",
    "              'under', 'university', 'unknown', 'unlikely', 'until', 'up', 'upon', 'use', 'used', \n",
    "              'useful', 'using', 'usually', 'value', 'various', 'vary', 've', 'very', 'via', 'view', \n",
    "              'was', 'wasn', \"wasn't\", 'we', 'well', 'were', 'weren', \"weren't\", 'what', 'whatever', \n",
    "              'when', 'where', 'whereas', 'whether', 'which', 'while', 'who', 'whom', 'why', 'wiley', \n",
    "              'will', 'with', 'within', 'without', 'won', \"won't\", 'work', 'would', 'wouldn', \n",
    "              \"wouldn't\", 'y', 'yield', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours',\n",
    "              'yourself', 'yourselves', 'involve', 'investigate', 'apparent', 'identify', 'assess']\n",
    "           \n",
    "\n",
    "\n",
    "def preprocess_text(doc, n_grams='one'):\n",
    "    \"\"\"\n",
    "    Pre-processing using TextBlob: \n",
    "    tokenizing, converting to lower-case, and lemmatization based on POS tagging, \n",
    "    removing stop-words, and retaining tokens greater than length 2\n",
    "\n",
    "    We can also choose to include n_grams (n = 1,2,3) in the final output\n",
    "\n",
    "    Argument(s): 'doc' - a string of words or sentences.\n",
    "                 'n_grams' - one: only unigrams (tokens consisting of one word each)\n",
    "                           - two: only bigrams\n",
    "                           - two_plus: unigrams + bigrams\n",
    "                           - three: only trigrams \n",
    "                           - three_plus: unigrams + bigrams + trigrams\n",
    "\n",
    "    Output: 'reuslt_singles' - a list of pre-processed tokens (individual words) of each sentence in 'doc'\n",
    "            'result_ngrams' - a list of pre-processed tokens (including n-grams) of each sentence in 'doc'\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    blob = TextBlob(doc).lower()\n",
    "#     lang = blob.detect_language()\n",
    "#     print(lang)\n",
    "#     if lang != 'en':\n",
    "#         blob = blob.translate(to = 'en')\n",
    "\n",
    "    result_singles = []\n",
    "\n",
    "    tag_dict = {\"J\": 'a',  # Adjective\n",
    "                \"N\": 'n',  # Noun\n",
    "                \"V\": 'v',  # Verb\n",
    "                \"R\": 'r'}  # Adverb\n",
    "\n",
    "    # For all other types of parts of speech (including those not classified at all)\n",
    "    # the tag_dict object maps to 'None'\n",
    "    # the method w.lemmatize() defaults to 'Noun' as POS for those classified as 'None'\n",
    "\n",
    "    for sent in blob.sentences:\n",
    "\n",
    "        words_and_tags = [(w, tag_dict.get(pos[0])) for w, pos in sent.tags]\n",
    "        lemmatized_list = [w.lemmatize(tag) for w, tag in words_and_tags]\n",
    "\n",
    "        for i in range(len(lemmatized_list)):\n",
    "\n",
    "            if lemmatized_list[i] not in stop_words and len(lemmatized_list[i].lower()) > 2 and not lemmatized_list[i].isdigit():\n",
    "                result_singles.append(lemmatized_list[i].lower())\n",
    "\n",
    "    result_bigrams = ['_'.join(x) for x in ngrams(result_singles, 2)]\n",
    "\n",
    "    result_bigrams = [\n",
    "        token for token in result_bigrams if token != 'psychological_association']\n",
    "\n",
    "    result_trigrams = ['_'.join(x) for x in ngrams(result_singles, 3)]\n",
    "    result_two_plus = result_singles + result_bigrams\n",
    "    result_three_plus = result_singles + result_bigrams + result_trigrams\n",
    "\n",
    "    if n_grams == 'one':\n",
    "        result = result_singles\n",
    "    elif n_grams == 'two':\n",
    "        result = result_bigrams\n",
    "    elif n_grams == 'three':\n",
    "        result = result_trigrams\n",
    "    elif n_grams == 'two_plus':\n",
    "        result = result_two_plus\n",
    "    elif n_grams == 'three_plus':\n",
    "        result = result_three_plus\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-19T13:31:12.033638Z",
     "iopub.status.busy": "2020-05-19T13:31:12.033372Z",
     "iopub.status.idle": "2020-05-19T13:31:12.037699Z",
     "shell.execute_reply": "2020-05-19T13:31:12.037085Z",
     "shell.execute_reply.started": "2020-05-19T13:31:12.033615Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_frequency(processed_text_list):\n",
    "    \"\"\"\n",
    "    Using a built-in NLTK function that generates tuples\n",
    "    We get the frequency distribution of all words/n-grams in a tokenized list\n",
    "    We can get the proportion of the the token as a fraction of the total corpus size  ----> N/A\n",
    "    We can also sort these frequencies and proportions in descending order in a dictionary object ----> N/A\n",
    "\n",
    "    Argument(s): 'processed_text_list' - A list of pre-processed tokens\n",
    "\n",
    "    Output(s): freq_dict - A dictionary of tokens and their respective frequencies in descending order\n",
    "    \"\"\"\n",
    "    # prop_dict - A dictionary of tokens and their respective proportions as a fraction of the total corpus\n",
    "    # combined_dict - A dictionary whose values are both frequencies and proportions combined within a list\n",
    "    # \"\"\"\n",
    "\n",
    "    word_frequency = FreqDist(word for word in processed_text_list)\n",
    "\n",
    "#     sorted_counts = sorted(word_frequency.items(), key = lambda x: x[1], reverse = True)\n",
    "#     freq_dict = dict(sorted_counts)\n",
    "    freq_dict = dict(word_frequency)\n",
    "#     prop_dict = {key : freq_dict[key] * 1.0 / sum(freq_dict.values()) for key, value in freq_dict.items()}\n",
    "#     combined_dict = {key : [freq_dict[key], freq_dict[key] * 1.0 / sum(freq_dict.values())] for key, value in freq_dict.items()}\n",
    "\n",
    "    return freq_dict  # , prop_dict, combined_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-19T14:03:34.747754Z",
     "iopub.status.busy": "2020-05-19T14:03:34.747554Z",
     "iopub.status.idle": "2020-05-19T14:03:34.755153Z",
     "shell.execute_reply": "2020-05-19T14:03:34.754583Z",
     "shell.execute_reply.started": "2020-05-19T14:03:34.747733Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_vocab_dictionary(vocab_column):\n",
    "    \"\"\"\n",
    "    Takes any number of token frequency dictionaries and merges them while summing \n",
    "    the respective frequencies and then calculates the proportion of the the tokens \n",
    "    as a fraction of the total corpus size and saves to text and CSV files\n",
    "\n",
    "\n",
    "    Argument(s): vocab_column - A column of dictionary objects\n",
    "\n",
    "    Output(s): merged_combined_dict - A list object containing the frequencies of all\n",
    "               merged dictionary tokens along with their respective proportions\n",
    "    \"\"\"\n",
    "\n",
    "    merged_freq_dict = {}\n",
    "    for dictionary in vocab_column:\n",
    "        for key, value in dictionary.items():  # d.items() in Python 3+\n",
    "            merged_freq_dict.setdefault(key, []).append(1)\n",
    "\n",
    "    for key, value in merged_freq_dict.items():\n",
    "        merged_freq_dict[key] = sum(value)\n",
    "\n",
    "    sorted_merged_freq_dict = sorted(\n",
    "        merged_freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#     total_sum = sum(merged_freq_dict.values())\n",
    "#     merged_prop_dict = {key : merged_freq_dict[key] * 1.0 / total_sum for key, value in merged_freq_dict.items()}\n",
    "#     merged_combined_dict = {key : [merged_freq_dict[key], (merged_freq_dict[key] * 1.0 / total_sum)] for key, value in merged_freq_dict.items()}\n",
    "\n",
    "    return sorted_merged_freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-19T13:33:47.770293Z",
     "iopub.status.busy": "2020-05-19T13:33:47.770071Z",
     "iopub.status.idle": "2020-05-19T13:33:47.773171Z",
     "shell.execute_reply": "2020-05-19T13:33:47.772629Z",
     "shell.execute_reply.started": "2020-05-19T13:33:47.770271Z"
    }
   },
   "outputs": [],
   "source": [
    "def fix_eval_issue(doc):\n",
    "    return literal_eval(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-19T13:57:53.760567Z",
     "iopub.status.busy": "2020-05-19T13:57:53.760343Z",
     "iopub.status.idle": "2020-05-19T13:57:53.802531Z",
     "shell.execute_reply": "2020-05-19T13:57:53.801868Z",
     "shell.execute_reply.started": "2020-05-19T13:57:53.760545Z"
    }
   },
   "outputs": [],
   "source": [
    "mcl_data = pd.read_csv(\n",
    "    \"/Users/shreya/Desktop/Theta_plus/MCL_ncf_abstract_top10_csv/ncf_20_top10_1_title_abstract.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-19T13:57:54.700128Z",
     "iopub.status.busy": "2020-05-19T13:57:54.699876Z",
     "iopub.status.idle": "2020-05-19T13:57:54.710970Z",
     "shell.execute_reply": "2020-05-19T13:57:54.710418Z",
     "shell.execute_reply.started": "2020-05-19T13:57:54.700099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scp</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14949207</td>\n",
       "      <td>Cleavage of structural proteins during the ass...</td>\n",
       "      <td>Using an improved method of gel electrophoresi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84981840703</td>\n",
       "      <td>An Eight‐day Method for Screening Compounds ag...</td>\n",
       "      <td>SYNOPSIS. Intracardial inoculation of 1–10 mil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17184389</td>\n",
       "      <td>A rapid and sensitive method for the quantitat...</td>\n",
       "      <td>A protein determination method which involves ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23922515</td>\n",
       "      <td>Identification of the avian homologues of mamm...</td>\n",
       "      <td>Two mAb were produced against chicken T cells....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33751553722</td>\n",
       "      <td>Nitrogen to Protein Conversion Factor for Ten ...</td>\n",
       "      <td>The main two kinds of usual nitrogen to protei...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           scp                                              title  \\\n",
       "0     14949207  Cleavage of structural proteins during the ass...   \n",
       "1  84981840703  An Eight‐day Method for Screening Compounds ag...   \n",
       "2     17184389  A rapid and sensitive method for the quantitat...   \n",
       "3     23922515  Identification of the avian homologues of mamm...   \n",
       "4  33751553722  Nitrogen to Protein Conversion Factor for Ten ...   \n",
       "\n",
       "                                       abstract_text  \n",
       "0  Using an improved method of gel electrophoresi...  \n",
       "1  SYNOPSIS. Intracardial inoculation of 1–10 mil...  \n",
       "2  A protein determination method which involves ...  \n",
       "3  Two mAb were produced against chicken T cells....  \n",
       "4  The main two kinds of usual nitrogen to protei...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcl_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-19T13:58:11.033986Z",
     "iopub.status.busy": "2020-05-19T13:58:11.033761Z",
     "iopub.status.idle": "2020-05-19T13:58:13.726585Z",
     "shell.execute_reply": "2020-05-19T13:58:13.725934Z",
     "shell.execute_reply.started": "2020-05-19T13:58:11.033961Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e3a86a1ff645a1a20a83e39db19b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=78.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d3ecad81f94e82be2dc76f5a5dfcef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=78.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_text = mcl_data.copy()\n",
    "data_text['scp'] = data_text.astype('str')\n",
    "data_text['processed_title'] = data_text['title'].swifter.apply(\n",
    "    preprocess_text)\n",
    "data_text['processed_abstract'] = data_text['abstract_text'].swifter.apply(\n",
    "    preprocess_text)\n",
    "\n",
    "data_text['processed_all_text'] = data_text['processed_title'] + data_text['processed_abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-19T13:58:23.674991Z",
     "iopub.status.busy": "2020-05-19T13:58:23.674767Z",
     "iopub.status.idle": "2020-05-19T13:58:23.706781Z",
     "shell.execute_reply": "2020-05-19T13:58:23.706198Z",
     "shell.execute_reply.started": "2020-05-19T13:58:23.674969Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99439f6ff8f34b719169be15ea8fc7e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=78.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_text['processed_all_text_frequencies'] = data_text['processed_all_text'].swifter.apply(\n",
    "    get_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-19T13:58:34.530361Z",
     "iopub.status.busy": "2020-05-19T13:58:34.530132Z",
     "iopub.status.idle": "2020-05-19T13:58:34.534914Z",
     "shell.execute_reply": "2020-05-19T13:58:34.534365Z",
     "shell.execute_reply.started": "2020-05-19T13:58:34.530339Z"
    }
   },
   "outputs": [],
   "source": [
    "data_all_text_frequency = merge_vocab_dictionary(\n",
    "    data_text['processed_all_text_frequencies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-19T14:04:01.145595Z",
     "iopub.status.busy": "2020-05-19T14:04:01.145385Z",
     "iopub.status.idle": "2020-05-19T14:04:01.163386Z",
     "shell.execute_reply": "2020-05-19T14:04:01.161939Z",
     "shell.execute_reply.started": "2020-05-19T14:04:01.145573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of articles:  78\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-c8b34d00b7ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Total number of articles: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmax_freq_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_all_text_frequency\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmax_freq_token_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_all_text_frequency\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "print('Total number of articles: ', len(data_text))\n",
    "\n",
    "max_freq_tokens = [i[0] for i in data_all_text_frequency[:50]]\n",
    "max_freq_token_counts = [i[1] for i in data_all_text_frequency[:50]]\n",
    "fig = plt.figure(figsize=(15, 7))\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.bar(max_freq_tokens, max_freq_token_counts)\n",
    "plt.title('Top 50 Most Frequent Tokens in ncf_20_1_title')\n",
    "plt.ylabel('Frequncy')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_bow(doc):\n",
    "\n",
    "#     bow_corpus = id2word_n_1.doc2bow(doc)\n",
    "\n",
    "#     return bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "mcl_dictionary = gensim.corpora.Dictionary(\n",
    "    data_text.processed_all_text.tolist())\n",
    "\n",
    "# Create Corpus\n",
    "mcl_all_text = data_text.processed_all_text.tolist()\n",
    "\n",
    "mcl_corpus = [' '.join(text) for text in mcl_all_text]\n",
    "\n",
    "# Term Document Frequency\n",
    "mcl_bow_corpus = [mcl_dictionary.doc2bow(text) for text in mcl_all_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 1\n",
    "\n",
    "# Build LDA model\n",
    "mcl_lda = gensim.models.ldamodel.LdaModel(corpus=mcl_bow_corpus,\n",
    "                                          id2word=mcl_dictionary,\n",
    "                                          num_topics=num_topics,\n",
    "                                          random_state=100,\n",
    "                                          update_every=1,\n",
    "                                          chunksize=100,\n",
    "                                          passes=10,\n",
    "                                          alpha='auto',\n",
    "                                          per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mcl_lda.print_topics(num_words=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize the topics\n",
    "# pyLDAvis.enable_notebook()\n",
    "# vis = pyLDAvis.gensim.prepare(mcl_lda, mcl_bow_corpus, mcl_dictionary)\n",
    "# vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_tokens(lda_model=mcl_lda, num_tokens=30):\n",
    "    \"\"\"\n",
    "    In current state, only works for num_topics = 1\n",
    "    \"\"\"\n",
    "\n",
    "    x = lda_model.show_topics(num_words=num_tokens, formatted=False)\n",
    "\n",
    "    topic_tokens = []\n",
    "    for topics in x:\n",
    "        for word, prob in topics[1]:\n",
    "            topic_tokens.append(word)\n",
    "\n",
    "    return topic_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Similarities\n",
    "def token_to_doc_similarity(doc, compare='lda', num_tokens=30):\n",
    "\n",
    "    if compare == 'lda':\n",
    "        compare_tokens = get_lda_tokens(mcl_lda, num_tokens)\n",
    "    elif compare == 'frequency':\n",
    "        compare_tokens = [i[0] for i in data_all_text_frequency[:num_tokens]]\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    count_vec_compare = vectorizer.fit_transform([' '.join(compare_tokens)])\n",
    "    count_vec_doc = vectorizer.transform([' '.join(doc)])\n",
    "\n",
    "    cos_sim = cosine_similarity(count_vec_compare, count_vec_doc)\n",
    "\n",
    "    return cos_sim[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text['lda_similarity'] = data_text['processed_all_text'].swifter.apply(\n",
    "    token_to_doc_similarity, args=('lda', 30))\n",
    "data_text['max_frequency_similarity'] = data_text['processed_all_text'].swifter.apply(\n",
    "    token_to_doc_similarity, args=('frequency', 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The total number of articles is: ', len(data_text))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(data_text['lda_similarity'])\n",
    "plt.suptitle('Distribution of Cosine Similarity Values')\n",
    "plt.title('Between Articles and Top 30 LDA Tokens')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "print('Number of articles with cos_sim > 0.3: ', len(\n",
    "    data_text[data_text['lda_similarity'] > 0.3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The total number of articles is: ', len(data_text))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(data_text['max_frequency_similarity'])\n",
    "plt.suptitle('Distribution of Cosine Similarity Values')\n",
    "plt.title('Between Articles and Top 30 Most Frequent Tokens')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "print('Number of articles with cos_sim > 0.3: ', len(\n",
    "    data_text[data_text['max_frequency_similarity'] > 0.2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA gives us:\n",
    "\n",
    "# - A distribution over topics for each document\n",
    "# - A distribution over words for each topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_text['article_bow'] = data_text['processed_all_text'].swifter.apply(make_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row_index, row in data_text_n_1[10:20].iterrows():\n",
    "#     row = row.copy()\n",
    "#     new_doc = [mcl_dictionary.doc2bow(row['processed_all_text'])]\n",
    "# #     print(new_doc)\n",
    "#     lda_result = gensim.models.ldamodel.LdaModel(corpus = new_doc,\n",
    "#                                            id2word = mcl_dictionary,\n",
    "#                                            num_topics = num_topics,\n",
    "#                                            random_state = 100,\n",
    "#                                            update_every = 1,\n",
    "#                                            chunksize = 100,\n",
    "#                                            passes = 10 ,\n",
    "#                                            alpha = 'auto',\n",
    "#                                            per_word_topics = True)\n",
    "\n",
    "\n",
    "# #     x = lda_result.show_topics(num_words=30,formatted=False)\n",
    "\n",
    "# #     doc_topics =[]\n",
    "# #     for topics in x:\n",
    "# #         for word, prob in topics[1]:\n",
    "# #              doc_topics.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcl_total_cluster_corpus = [' '.join(mcl_corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "mcl_total_cluster_count_mat = count_vectorizer.fit_transform(mcl_total_cluster_corpus)\n",
    "mcl_doc_term_count_mat = count_vectorizer.transform(mcl_corpus)\n",
    "\n",
    "prob_transformer = TfidfTransformer(norm='l1', use_idf=False, smooth_idf=False)\n",
    "\n",
    "mcl_total_cluster_prob_mat = prob_transformer.fit_transform(mcl_total_cluster_count_mat)\n",
    "mcl_doc_term_prob_mat = prob_transformer.fit_transform(mcl_doc_term_count_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcl_total_cluster_count_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcl_doc_term_count_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcl_total_cluster_prob_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcl_doc_term_prob_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text['doc_term_prob'] = mcl_doc_term_prob_mat.toarray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance.jensenshannon([1.0, 0.0, 0.0], [0.0, 1.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcl_total_cluster_prob_vec = mcl_total_cluster_prob_mat.toarray().tolist()[0]\n",
    "\n",
    "\n",
    "def calculate_jsd(prob_array, cluster_prob_vec=mcl_total_cluster_prob_vec):\n",
    "\n",
    "    jsd = distance.jensenshannon(prob_array, cluster_prob_vec)\n",
    "\n",
    "    return jsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text['JSD'] = data_text['doc_term_prob'].swifter.apply(calculate_jsd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The total number of articles is: ', len(data_text))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(data_text['JSD'])\n",
    "plt.suptitle('Distribution of JSD Values')\n",
    "# plt.title('Between Articles and Top 30 Most Frequent Tokens')\n",
    "plt.xlabel('JSD')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# print('Number of articles with cos_sim > 0.3: ', len(data_text[data_text['max_frequency_similarity']>0.2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
