\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}
\modulolinenumbers[5]

\journal{Heliyon}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%
%George's Additions
%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{subcaption}
\usepackage{changepage}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsmath,amssymb}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

%% Misc. Packages
\usepackage[final]{changes}
\definechangesauthor[name={George Chacko}, color=orange]{gc}
\setremarkmarkup{(#2)}
\usepackage{listings}
\usepackage{url}


\begin{document}

\begin{frontmatter}

\title{ERNIE: A Knowledge Platform to Support Research Assessment}
%\tnotetext[mytitlenote]{This document is a collaborative effort}

%% Group authors per affiliation:
%% or include affiliations in footnotes:
\author[nl]{Samet Keserci}
\author[nl]{Avon Davey}
\author[ca]{Di Cross}
\author[gi]{Alexander R. Pico}
\author[nl]{Dmitriy Korobskiy}
\author[nl]{George Chacko \corref{cor1}}
\ead{netelabs@nete.com}

\cortext[cor1]{Corresponding author}
%\fntext[fn1]{Current address: Facebook Inc., Menlo Park, CA, USA}


\address[nl]{NETE Labs, NET ESolutions Corporation, McLean, VA, USA}
\address[ca]{Research Data Science \& Evaluation, Clarivate Analytics, USA}
\address[gi]{Gladstone Institutes, San Francisco, CA, USA}

\raggedright

\begin{abstract}

Data mining coupled to network analysis has been successfully used as a digital methodology to study research collaborations and knowledge flow associated with drug development. To enable quantitative studies based on this approach, we have developed Enhanced Research Network Information Environment (ERNIE), a scalable cloud-based knowledge platform that integrates free data drawn from public sources as well as licensed data from commercially available ones. Analytical workflows in ERNIE are partially automated to enable expert input at critical stages. To facilitate adoption, reuse and extensibility, ERNIE is built with open source tools. and a modular design enables facile addition, deletion, or substitution of data sources. To demonstrate the capabilities of ERNIE, we report the results of seven case studies that span opioid addiction, pharmacogenomics, target discovery, behavioral interventions, and drug development. In these studies, we mine and analyze data from policy documents, regulatory approvals, research grants, bibliographic and patent databases, and clinical trials, to document collaborations and identify influential research accomplishments. ERNIE is presented as an accessible template for repositories that can be used to support expert qualitative assessments, while offering burden reduction through automation and access to integrated data.

\end{abstract}

%\begin{keyword}
%\end{keyword}

\end{frontmatter}

\linenumbers
\raggedright

\section*{Introduction}

The risk of research evaluation being driven by data and metrics with insufficient consideration of balanced judgment has driven the formulation of  sound principles to advise practice~\cite{LeidenManifesto2015}. Data are still critical for evaluation, however, and utilities for capture, integration, curation, archival, and analysis that offer coverage and burden reduction can only assist the practice of research evaluation. Bibliographic data, often central to research evaluation, with the complementary use of administrative records such as government statistics and research funding are of considerable value~\cite{FedStat2017}. In this regard, Williams and colleagues have conducted network analysis of mined data from publicly available administrative, regulatory, and scientific resources to define relationships between scientific discoveries and major advances in medicine such as new drugs~\cite{Williams2015}. Extending this citation-centric approach, we have documented the scientific collaborations that extend across networks underlying the development of five independently developed therapeutics for cancer~\cite{Keserci2017}. Analysis of such networks supports new insights into collaboration, knowledge diffusion, and recursive learning \textit{(vide supra)}. 

Cloud computing has also evolved to the point where scalable computing services and analytical methods can be made accessible to a broad user population and exerts a democratizing effect on research activities~\cite{Barga2011}. To take advantage of these advances in infrastructure and methodology, we have developed Enhanced Research Network Informatics Environment (ERNIE), a knowledge platform for research evaluation that integrates publicly available data as well as data from commercial sources. Focused on the theme of data mining and network analysis and to demonstrate the utility of ERNIE, we have used its data to conduct seven case studies. In these studies, we mine and analyze data from policy documents, regulatory approvals, research grants, bibliographic and patent databases, and clinical trials, to document collaborations and identify influential research accomplishments. The framework used to mine data and integrate it into networks for analysis incorporates expert input is simple, flexible, can be further automated, adapted to a range of subjects beyond drug development, and used to support individual researchers, groups, and research organizations engaged in research assessment. The workflows for these case studies incorporate expert knowledge into defining a set of core documents from which citations can be extracted, linked to other records in ERNIE, and analyzed. The first two case studies serve to validate ERNIE through reproducing the results of prior studies~\cite{Williams2015} on ivacaftor and ipilimumab, breakthrough drugs used to treat cystic fibrosis and melanoma. The remaining case studies relate to the current national crisis on substance abuse and concern two drugs used to treat opioid addiction~\cite{Blumberg1973, Cowan1977}, a microarray system for pharmacogenomic profiling~\cite{deLeon2006}, a target discovery system originating from enzyme fragment complementation assays that has been used in opioid receptor studies~\cite{Khanna1989}, and a behavioral intervention for substance abuse~\cite{Botvin1980}. The datasets that result from these studies are made available for use by other researchers and the workflows used to generate them are also archived to permit reproduction of our results, as well as review and modification of the methods used to generate them.

\section*{Materials and Methods}

\emph{Infrastructure} ERNIE runs on two IaaS CentOS 7.4 Linux servers in the Microsoft Azure cloud. One server, a standard D8s v3 (8 vCPUs, 32 Gb RAM) with 8 Tb of attached SSD storage, acts as a central hub for data related processes and hosts data in a PostgreSQL 9.6 relational database. The second server, a standard DS4 (8 vCPUs, 28 Gb) with 1 Tb of attached SSD storage also runs CentOS 7.4 and has the Apache Solr 7.1 search server installed. The design of ERNIE includes a PaaS Apache Spark cluster with 9 nodes  that is provisioned, using HDInsight, on demand and  is used for compute intensive tasks with Apache Sqoop, PySpark (Spark Python API), and data in Parquet format. Access to ERNIE servers is secured through SSH tunneling and two-factor authentication, with a unique time-based code acting as one of the factors. Interactive users have access to Python, SQL, R, Java, and standard Linux utilities. JupyterHub has also been installed to enable remote access to the Jupyter notebook server for multiple users. 

\begin{figure}[!h]
%\begin{adjustwidth}{-0.5in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
\scalebox{0.99}
{
\begin{subfigure}{.5\textwidth}
  \centering
\includegraphics[width=.95\linewidth]{ERNIE_System_Context_2.pdf}
  \label{fig:sub1}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
 \includegraphics[width=.95\linewidth]{ERNIE_System_Context_1.pdf}
  \label{fig:sub2}
\end{subfigure}
}
\caption{{\bf System Context View} ERNIE runs on two IaaS CentOS 7.4 Linux servers in the Microsoft Azure cloud. \emph{Left Panel.} Users interactively access the environment through multifactor authentication (MFA). \emph{Right Panel.} ERNIE data is populated through automated custom ETL processes }
\label{fig: test}
%\end{adjustwidth}
\end{figure}

\emph{Data sources} Data in ERNIE are derived from both publicly available and commercial sources and are stored in a documented relational schema. Publicly available data are copied from ClinicalTrials.gov~\cite{ClinicalTrials2017},  the US Food and Drug Administration (FDA) Orange~\cite{OrangeBook2017} and Purple~\cite{PurpleBook2017} Books, the United States Patent Office~\cite{USPTO2017}, and NIH ExPORTER~\cite{NIHExporter2017}. Leased data are acquired from Clarivate Analytics and consist of the Web of Science Core Collection~\cite{WoS2017} and the Derwent World Patent Index~\cite{DWPI2017}. Web of Science, Patents, Clinical Trials, and Clinical Guidelines data are updated weekly. All other data sources are updated monthly.  Custom Extract-Transform-Load (ETL) scripts developed in Bash, Python, Java and SQL are used for initial loading as well as refreshes. Automated processes are managed by the Jenkins Continuous Integration server, which is used to (i) deploy scripts dedicated to data transformation (ii) direct construction of Solr indexes as new data is introduced into ERNIE (iii) build citation networks for case studies (iv) manage data refreses. All ERNIE code is shared as open-source under the MIT license and is available on GitHub~\cite{GithubERNIE2017}. Table 1 provides an overview of the total count of records in major data sources (excluding records from tables used to process data) in the schema in ERNIE. % latex table generated in R 3.4.3 by xtable 1.8-2 package
% Wed Jan  3 15:03:57 2018

\begin{table}[ht]
%\centering
\begin{tabular}{rll}
  \hline
 & Data Source & No of Records \\ 
  \hline
1 & Web Of Science Core Collection & 2.5 x$10^{9}$\\ 
  2 & Derwent US Patents & 2.4 x$10^{8}$\\
  3 & NIH ExPORTER & 7.1 x$10^{6}$ \\ 
  4 & NLM Clinical Trials & 8.6 x$10^{6}$ \\ 
  5 &  FDA & 5.0 x$10^{4}$\\  
  6 & AHRQ Clinical Guidelines & 2.6 x$10^{3}$ \\ 
   \hline
\end{tabular}
\caption {Summed counts of records in ERNIE grouped by major data sources}
\end{table}

\emph{Data architecture and governance.} ERNIE data is drawn from several upstream sources of varying quality. Cleansing approaches in place are designed to favor data quality and normalized data over completeness. Key aspects of ERNIE's data architecture and governance are (i) De-duplication to eliminate logically duplicate records, that is records with identical primary or unique keys. In ERNIE duplicate records from such a group are deleted while leaving in place a single representative record (preferring a record which is updated last) (ii) Normalization to establish foreign key relationships between children and parent entities and eliminate orphans: child records with non-existing parents. (iii) Schema permanence to improve availability of data and simplify data schema management (iv) Data Stewardship to assign responsibility for quality of a particular data subject area. Data stewards are notified when a data issue is detected and drive the workflows necessary for data issue remediation. They are also typically responsible for communicating changes in data structures.

\emph{Workflows.} The core workflow in ERNIE can be divided into five stages (i) Source document identification, in which a set of source documents relevant to the question being asked is assembled  (ii) Citation extraction, in which text references to scientific publications and patents are matched to unique identifiers such as PubMed ID, UT (Web of Science Accession Number), or US patent number (iii) Amplification and linking, in which publications and patents linked by citation to extracted references are identified through citation records in the Web of Science and linked to other records in ERNIE (iv) Network analysis, in which the data assembled through the preceding stages are represented as a network and influential nodes are identified (v) Visualization of the network data to provide insight and stimulate further investigation.

\emph {Source document identification.} A set of source documents relevant to the question being asked is assembled and searched for citations to scientific literature. The process involves input from knowledgeable persons, is subjective, and centers around regulatory documents made available by the FDA, documents on government web sites such as the office of The Surgeon General, the White House, and the Office of National Drug Control Policy, and technical documents on corporate web sites such as those of DiscoverX and Affymetrix. For example, the The Surgeon General's Report on Alcohol, Health, and Addiction~\cite{SurgGen2017} and The President's Commission on Combating Drug Addiction and The Opioid Crisis~\cite{PresidComm2017} were identified as key source documents relevant to the Life Skills Training Case study in this article. 

\emph {Citation extraction and matching.} References to scientific literature and patents found in the source documents are matched to unique identifiers using manual or partially-automated in the Web of Science Core Collection~\cite{Keserci2017}.  Manual extraction involves searching for text strings that represent references of interest and matching those to unique identifiers in PubMed and/or Web of Science. For example, US6984720 B1, the core patent relevant to ipilimumab contained 52 references to scientific publications listed under non-patent citations. All 52 of them were manually matched to PubMed identifiers (pmid) using the PubMed website. In a second approach, a partially-automated one, Apache Solr is used to to create and store indexes of ERNIE data that can facilitate user searches that return informative, ranked, and scored query results and form the basis for a semi-automated process that takes advantage of indexed Web of Science publications in ERNIE. In this process, a text file containing references is passed as input to a Python script and a user manually selects from the output hit list that includes citation information, a Web of Science Accession Number (UT), and the numeric output of the Solr scoring function. The latter process offers scalability and can process 30-40 references per minute. A combination of these approaches was used in the seven case studies described. The product of citation extraction and matching is termed the seedset.

\emph{Amplification.} Once unique identifiers to scientific literature are assembled in the seedset, cited and/or citing references in the Web of Science Core Collection are identified using the wos\_references table (see schema). Only those records with an entry in the wos\_publications table (see schema) are considered when counting citing or cited references. These are then linked to authors using Web of Science data. Wherever possible, UTs are mapped to pmids using a lookup table and then to grants using data linking publications to grant support from NIH ExPORTER. 

\emph{Network analysis.} For network  analysis, 4 input files are used that are derived from the amplification stage (i) Article and Author edge list, a mapping of publications to authors. (ii) Article and first generation citations, a list of seedset and a single generation of cited or citing references derived from the amplification cycle. (iii) Publication and publication edge list, the citation network after amplification stored as source-target citation pairs. (iv) Publication and published year: A mapping of publications to their year of publication. Once these four files are generated, the network analyzer code generates 7 output files:

\begin{enumerate}
  \item Author scores based on PubMed and WoS. These are mappings of publications to their authors using either PubMed or Web of Science data. 
  \item Article scores based on PubMed and WoS: These one  contains the information of seed set and its first generation without inter citation information.
  \item Edge List of Articles for Visualization based on PubMed and WoS. These is the citation network of the publications after amplification.
  \item Descriptive statistics: This file contains  basic descriptive statistics about articles and authors such as counts, max citations, and number of lost nodes during the various mapping. 
\end{enumerate}

Sample metrics for Authors are number of articles, total citation and propagated in-degree rank (PIR) ~\cite{Williams2015}, and for articles we calculate the in network citation, weighted citation (propagated citation with depth = 2). In Each case study,  we calculate the scores as below:

Given the articles-to-articles citation relation, let matrix $G$ be a $n$x$n$ matrix where $n$ represent the number of articles in the network be a Matrix representation of the Network. We define 
the components $g_{ij}$ of the matrix $G$ as follows,

\begin{equation*}
g_{ij} = \begin{cases}
1 &\text{$i^{th}$ articles cites $j^{th}$ article}\\
0 &\text{Otherwise}
\end{cases}
\end{equation*}

Then score of of an article  $\mathfrak{p}$ is denoted as $s(\mathfrak{p})$ and is represented by,

\begin{equation*}
s(\mathfrak{p}) = [u*G]_{j} = \sum_k^n  u_k \cdot g_{jk}
\end{equation*}

where $u$ is a weight vector. Here, for different weight vector, we will get different measurements.
For example, for  $u=<1,1,1...,1>$,
\begin{equation*}
s(\mathfrak{p}_j) = [u*G]_{j} = \sum_k^n  u_k \cdot g_{jk}=\sum_k^n  1\cdot g_{jk}
\end{equation*}
which is just citation of the article $\mathfrak{p}_j$, denoted by $cit(\mathfrak{p}_j)$. There are various weight vector we could use such as weight based on the publication year, weight based on over all citation flow (~ Page rank), or we could have have normalized weight vector by it is average citation score. In these case studies we used one more weight vector to produce William et.all result. This new weight enable us not to count the number of citing papers, but also consider the citation based quality of citing papers. Considering the weight vector to be $u=<cit(\mathfrak{p}_1),cit(\mathfrak{p}_2),...,cit(\mathfrak{p}_n)>$, then weighted citation of a papers according to this weight vector will be

\begin{equation*}
s(\mathfrak{p}_j) = [u*G]_{j} = \sum_k^n  u_k \cdot g_{jk}=\sum_k^n  cit(g_k)\cdot g_{jk} =\sum_k^n  \{\sum_m^n  g_{km}\} \cdot g_{jk}
\end{equation*}
which could be translated as the total sum of citation of all citing papers. Next if we add self citation to above quantity we will get Pico type article scoring metric. i.e

 
 \begin{equation}
 \begin{split}
s(\mathfrak{p}_j)+cit(\mathfrak{p}_j) & = \sum_k^n   \{\sum_m^n  g_{km}\} \cdot g_{jk} +cit(\mathfrak{p}_j)  \\
						       & =\sum_k^n   \{\sum_m^n  g_{km}\} \cdot g_{jk}+ \sum_k^n  g_{jk} \\
                                                          & =\sum_k^n   \{ 1+ \sum_m^n  g_{km} \} \cdot g_{jk}
\end{split}                                                          
\end{equation}

As is seen above if we refine the weight vector to be as, 
\begin{equation}\label{wu}
u = < 1+ \sum_m^n  g_{1m}  ,  1+  \sum_m^n  g_{2m} , ...,  1 +  \sum_m^n  g_{nm} >
\end{equation}
then  we will get article scores method developed in Williams et.all. Similary we could use weight vector based on h-index, e-index of authors, normalized citation count, sum of a h-index of the authors etc. We leave choice of metrics to users.

Next we calculate the scores of an authors $\mathfrak{a}$ as follows. Let matrix $\mathcal {A}$ be represent the Author publications relationship  by the following definition.

\begin{equation}
a_{ij} = \begin{cases}
1 &\text{$i^{th}$ Author wrotes $j^{th}$ article}\\
0 &\text{Otherwise}
\end{cases}
\end{equation}

Then the score of the author $\mathfrak{a}_j$ will be defined as 
\begin{equation}
S(\mathfrak{a}_j) =  \sum_k^n a_{jk} \cdot  s(p_k) 
\end{equation}

where $s(p_k)$ denote the publication score for any given weight $u$. Here if we choose weight $u$ to be $ <1,1,1,,,,1>$ then resulting score will be just total citation count of the Author $\mathfrak{a}$ in the network. Whereas choosing weight function as in ( \ref{wu} ) gives the PIR score  of an author.

\emph{Visualizations} Input and end product

\section*{Results and Discussion}

Fig. 1 -Architecture

Table 2- List of Case Studies

Table 2- seedset, cited references, grants, etc for all 7 

Table 3- NIH support- other support from WoS?

Figure 1- 

What can we say about substance abuse and opioid addiction. 

\section*{Acknowledgments} Research and development reported in this publication was supported by Federal funds from the National Institute on Drug Abuse, National Institutes of Health, US Department of Health and Human Services, under Contract No. HHSN271201700053C. The content of this publication is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.

\section*{References}

\bibliography{chacko}

\begin{figure}[!h]
%\begin{adjustwidth}{-0.5in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
\scalebox{0.99}
{
\begin{subfigure}{.5\textwidth}
  \centering
%  \includegraphics[width=.95\linewidth]{}
  \label{fig:sub1}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
 % \includegraphics[width=.95\linewidth]{}
  \label{fig:sub2}
\end{subfigure}
}
\caption{{\bf Intersecting Publications in Five Networks} Intersections were calculated across all five networks for the first generation of references (citing\_pmids) and
as well as for the second generation of references (cited\_sids) and displayed as Venn diagrams. \emph{Left Panel.} No first generation publications are observed common to all five networks. A single publication is cited in four of five networks. \emph{Right Panel.} 14  publications are common to all five networks. Abbreviations: alem (Alemtuzumab), imat (Imatinib), nela (Nelarabine), ramu (Ramucirumab), suni(Sunitinib)}
\label{fig: test}
%\end{adjustwidth}
\end{figure}

\begin{figure}[!h]
%\begin{adjustwidth}{-0.5in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
%\scalebox{1.3}{
%\includegraphics[scale=0.1]{cy_core14_v2csv_5b.png}}
\caption{{\bf Core Publications in Networks}  The outer arcs of blue nodes identifies first generation publications (citing\_sid) for each therapeutic. Nodes in the inner ring are sized by a gradient proportion to total degree count with an upper limit of 30 and are colored by a gradient proportional to the number of drug connections (2 to 5). 14  publications are common to all five networks (Table 3) and are colored red. The remaining nodes in the inner ring connect to between 2 and 4 drugs each and are labeled accordingly. Abbreviations: alem (Alemtuzumab), imat (Imatinib), nela (Nelarabine), ramu (Ramucirumab), suni(Sunitinib).}
\label{fig2}
%\end{adjustwidth}
\end{figure}

\end{document}